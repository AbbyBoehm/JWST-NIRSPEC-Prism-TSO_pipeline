{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Confirm that the environment is correct.\")\n",
    "print(sys.executable)\n",
    "\n",
    "import os\n",
    "os.environ['CRDS_PATH'] = './crds_cache/jwst_ops'\n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from jwst import datamodels as dm\n",
    "from jwst.pipeline import Detector1Pipeline, Spec2Pipeline\n",
    "import jwst\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import FormatStrFormatter as fsf\n",
    "from matplotlib.ticker import NullFormatter as nulf\n",
    "from matplotlib.ticker import AutoMinorLocator as aml\n",
    "\n",
    "import numpy as np\n",
    "from astropy import modeling\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clip\n",
    "\n",
    "import batman\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy import signal\n",
    "\n",
    "try:\n",
    "    from exotic_ld import StellarLimbDarkening as SLD\n",
    "    exotic_ld_available = True\n",
    "except:\n",
    "    print(\"EXoTiC-LD not found on this system, fixed limb darkening coefficients will not be available.\")\n",
    "    exotic_ld_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img(array, aspect=1, title=None, vmin=None, vmax=None, norm=None):\n",
    "    '''\n",
    "    Image plotting utility to plot the input 2D array.\n",
    "    \n",
    "    :param array: 2D array. Image you want to plot.\n",
    "    :param aspect: float. Aspect ratio. Useful for visualizing narrow arrays.\n",
    "    :param title: str. Title to give the plot.\n",
    "    :param vmin: float. Minimum value for color mapping.\n",
    "    :param vmax: float. Maximum value for color mapping.\n",
    "    :param norm: str. Type of normalisation scale to use for this image.\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(20, 25))\n",
    "    if norm == None:\n",
    "        im = ax.imshow(array, aspect=aspect, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    else:\n",
    "        im = ax.imshow(array, aspect=aspect, norm=norm, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    return fig, ax, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doStage1(filepath, outfile, outdir,\n",
    "             group_scale={\"skip\":False},\n",
    "             dq_init={\"skip\":False},\n",
    "             saturation={\"skip\":False},\n",
    "             superbias={\"skip\":False},\n",
    "             refpix={\"skip\":False},\n",
    "             linearity={\"skip\":False},\n",
    "             dark_current={\"skip\":False},\n",
    "             jump={\"skip\":True},\n",
    "             ramp_fit={\"skip\":False},\n",
    "             gain_scale={\"skip\":False},\n",
    "             one_over_f={\"skip\":False, \"bckg_rows\":[1,2,3,4,5,6,-1,-2,-3,-4,-5,-6], \"show\":False}\n",
    "             ):\n",
    "    '''\n",
    "    Performs Stage 1 calibration on one file.\n",
    "    \n",
    "    :param filepath: str. Location of the file you want to correct. The file must be of type *_uncal.fits.\n",
    "    :param outfile: str. Name to give to the calibrated file.\n",
    "    :param outdir: str. Location of where to save the calibrated file to.\n",
    "    :param group_scale, dq_init, saturation, etc.: dict. These are the dictionaries shown in the Detector1Pipeline()\n",
    "                                                   documentation, which control which steps are run and what parameters\n",
    "                                                   they are run with. Please consult jwst-pipeline.readthedocs.io for\n",
    "                                                   more information on these dictionaries.\n",
    "    :param one_over_f: dict. Keyword \"skip\" is a bool that sets whether or not to perform this step. Keyword\n",
    "                       \"bckg_rows\" contains list of integers that selects which rows of the array are used as\n",
    "                       background for 1/f subtraction. Keyword \"show\" is a bool that sets whether the first group\n",
    "                       of the first integration is shown as it is cleaned.\n",
    "    :return: a Stage 1 calibrated file *_rateints.fits saved to the outdir.\n",
    "    '''\n",
    "    # Create the output directory if it does not yet exist.\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    # Report initialization of Stage 1.\n",
    "    print(\"Performing Stage 1 calibration on file: \" + filepath)\n",
    "    print(\"Running JWST Stage 1 pipeline starting at GroupScale and stopping before Jump...\")\n",
    "    \n",
    "    # Collect timestamp to track how long this takes.\n",
    "    t0 = time.time()\n",
    "    with Detector1Pipeline.call(filepath,\n",
    "                                steps={\"group_scale\":group_scale,\n",
    "                                       \"dq_init\":dq_init,\n",
    "                                       \"saturation\":saturation,\n",
    "                                       \"superbias\":superbias,\n",
    "                                       \"refpix\":refpix,\n",
    "                                       \"linearity\":linearity,\n",
    "                                       \"dark_current\":dark_current,\n",
    "                                       \"jump\":jump,\n",
    "                                       \"ramp_fit\": {\"skip\": True}, \"gain_scale\": {\"skip\": True}}) as result:\n",
    "        print(\"Stage 1 calibrations up to step Jump resolved in %.3f seconds.\" % (time.time() - t0))\n",
    "        \n",
    "        if not one_over_f[\"skip\"]:\n",
    "            # Before we ramp_fit, we perform 1/f subtraction.\n",
    "            print(\"Performing pre-RampFit 1/f subtraction...\")\n",
    "            result.data = one_over_f_subtraction(result.data,\n",
    "                                                 bckg_rows=one_over_f[\"bckg_rows\"],\n",
    "                                                 show=one_over_f[\"show\"])\n",
    "        else:\n",
    "            print(\"Skipping 1/f subtraction...\")\n",
    "        \n",
    "        # Now we can resume Stage 1 calibration.\n",
    "        t02 = time.time()\n",
    "        print(\"Resuming Stage 1 calibrations through RampFit and GainScale steps.\"\n",
    "              \"\\nThe RampFit step can take several minutes to hours depending on how big your dataset is,\\n\"\n",
    "              \"so I suggest you find something else to do in the meantime. Anyways...\")\n",
    "        \n",
    "        result = Detector1Pipeline.call(result, output_file=outfile, output_dir=outdir,\n",
    "                                        steps={\"group_scale\": {\"skip\": True},\n",
    "                                               \"dq_init\": {\"skip\": True},\n",
    "                                               \"saturation\": {\"skip\": True},\n",
    "                                               \"superbias\": {\"skip\": True},\n",
    "                                               \"refpix\": {\"skip\": True},\n",
    "                                               \"linearity\": {\"skip\": True},\n",
    "                                               \"dark_current\": {\"skip\": True},\n",
    "                                               \"jump\": {\"skip\": True},\n",
    "                                               \"ramp_fit\":ramp_fit,\n",
    "                                               \"gain_scale\":gain_scale})\n",
    "        print(\"Finished final steps of Stage 1 calibrations in %.3f minutes.\" % ((time.time()-t02)/60))\n",
    "    print(\"File calibrated and saved.\")\n",
    "    print(\"Stage 1 calibrations completed in %.3f minutes.\" % ((time.time() - t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_over_f_subtraction(data, bckg_rows, show):\n",
    "    '''\n",
    "    Performs 1/f subtraction on the given array.\n",
    "    Adapted from routine developed by Trevor Foote (tof2@cornell.edu).\n",
    "    \n",
    "    :param data: 4D array. Array of integrations x groups x rows x cols.\n",
    "    :param bckg_rows: list of integers. Indices of the rows to use as the background region.\n",
    "    :param show: bool. Whether to show the cleaned frames. For inspection of whether this\n",
    "                 this step is working properly.\n",
    "    :return: 4D array that has been subjected to 1/f subtraction.\n",
    "    '''\n",
    "    # Time this step.\n",
    "    t0 = time.time()\n",
    "    for i in range(np.shape(data)[0]): # for each integration\n",
    "        for g in range(np.shape(data)[1]): # for each group\n",
    "            # Define the background region.\n",
    "            background_region = data[i, g, bckg_rows, :]\n",
    "            if (i == 0 and g == 0 and show):\n",
    "                fig, ax, im = img(np.log10(np.abs(background_region)), aspect=5, vmin=None, vmax=None, norm=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            \n",
    "            # Clean the background region of outliers, so that CRs aren't propagated through the array.\n",
    "            background_region = clean(background_region, 3, (5, 1)) # cleans on rows, rejecting at 3 sigma\n",
    "            if (i == 0 and g == 0 and show):\n",
    "                fig, ax, im = img(np.log10(np.abs(background_region)), aspect=5, vmin=None, vmax=None, norm=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            \n",
    "            # Define the mean background in each column and extend to a full-size array.\n",
    "            background = background_region.mean(axis=0)\n",
    "            background = np.array([background,]*np.shape(data)[2])\n",
    "            if (i == 0 and g == 0 and show):\n",
    "                fig, ax, im = img(np.log10(np.abs(background)), aspect=5, vmin=None, vmax=None, norm=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            if (i == 0 and g == 0 and show):\n",
    "                fig, ax, im = img(np.log10(np.abs(data[i, g, :, :])), aspect=5, vmin=None, vmax=None, norm=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            data[i, g, :, :] = data[i, g, :, :] - background\n",
    "\n",
    "            if (i == 0 and g == 0 and show):\n",
    "                fig, ax, im = img(np.log10(np.abs(data[i, g, :, :])), aspect=5, vmin=None, vmax=None, norm=None)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "        if (i%1000 == 0 and i != 0):\n",
    "            # Report every 1000 integrations.\n",
    "            elapsed_time = time.time()-t0\n",
    "            iterrate = i/elapsed_time\n",
    "            iterremain = np.shape(data)[0] - i\n",
    "            print(\"On integration %.0f. Elapsed time in this step is %.3f seconds.\" % (i, elapsed_time))\n",
    "            print(\"Average rate of integration processing: %.3f ints/s.\" % iterrate)\n",
    "            print(\"Estimated time remaining: %.3f seconds.\\n\" % (iterremain/iterrate))\n",
    "    print(\"1/f subtraction completed in %.3f seconds.\" % (time.time()-t0))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data, sigma, kernel):\n",
    "    '''\n",
    "    Cleans one 2D array with median spatial filtering.\n",
    "    Adapted from routine developed by Trevor Foote (tof2@cornell.edu).\n",
    "    \n",
    "    :param data: 2D array. Array that will be median-filtered.\n",
    "    :param sigma: float. Sigma at which to reject outliers.\n",
    "    :param kernel: tuple of odd int. Kernel to use for median filtering.\n",
    "    :return: cleaned 2D array.\n",
    "    '''\n",
    "    medfilt = signal.medfilt2d(data, kernel)\n",
    "    diff = data - medfilt\n",
    "    temp = sigma_clip(diff, sigma=sigma, axis=0)\n",
    "    mask = temp.mask\n",
    "    int_mask = mask.astype(float) * medfilt\n",
    "    test = (~mask).astype(float)\n",
    "    return (data*test) + int_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doStage2(filepath, outfile, outdir,\n",
    "             assign_wcs={\"skip\":False},\n",
    "             extract_2d={\"skip\":False},\n",
    "             srctype={\"skip\":False},\n",
    "             wavecorr={\"skip\":False},\n",
    "             flat_field={\"skip\":False},\n",
    "             pathloss={\"skip\":True},\n",
    "             photom={\"skip\":True},\n",
    "             resample_spec={\"skip\":True},\n",
    "             extract_1d={\"skip\":True}\n",
    "             ):\n",
    "    '''\n",
    "    Performs Stage 2 calibration on one file.\n",
    "    \n",
    "    :param filepath: str. Location of the file you want to correct. The file must be of type *_rateints.fits.\n",
    "    :param outfile: str. Name to give to the calibrated file.\n",
    "    :param outdir: str. Location of where to save the calibrated file to.\n",
    "    :param assign_wcs, background, extract2d, etc.: dict. These are the dictionaries shown in the Spec2Pipeline()\n",
    "                                                    documentation, which control which steps are run and what parameters\n",
    "                                                    they are run with. Please consult jwst-pipeline.readthedocs.io for\n",
    "                                                    more information on these dictionaries.\n",
    "    :return: a Stage 2 calibrated file *_calints.fits saved to the outdir.\n",
    "    '''\n",
    "    # Create the output directory if it does not yet exist.\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    print(\"Performing Stage 2 calibration on file: \" + filepath)\n",
    "    print(\"Running JWST Stage 2 pipeline for spectroscopic data. This stage is a pure wrapper for JWST Stage 2 with no mods. Anyways...\")\n",
    "    t0 = time.time()\n",
    "    result = Spec2Pipeline.call(filepath, output_file=outfile, output_dir=outdir,\n",
    "                                steps={\"assign_wcs\":assign_wcs,\n",
    "                                       \"extract_2d\":extract_2d,\n",
    "                                       \"srctype\":srctype,\n",
    "                                       \"wavecorr\":wavecorr,\n",
    "                                       \"flat_field\":flat_field,\n",
    "                                       \"pathloss\":pathloss,\n",
    "                                       \"photom\":photom,\n",
    "                                       \"resample_spec\":resample_spec,\n",
    "                                       \"extract_1d\":extract_1d})\n",
    "    print(\"File calibrated and saved.\")\n",
    "    print(\"Stage 2 calibrations completed in %.3f minutes.\" % ((time.time() - t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doStage3(filesdir, outdir,\n",
    "             trace_aperture={\"hcut1\":0,\n",
    "                             \"hcut2\":-1,\n",
    "                             \"vcut1\":0,\n",
    "                             \"vcut2\":-1},\n",
    "             frames_to_reject = [],\n",
    "             loss_stats_step={\"skip\":False},\n",
    "             mask_flagged_pixels={\"skip\":False},\n",
    "             iteration_outlier_removal={\"skip\":False, \"n\":2, \"sigma\":10},\n",
    "             spatialfilter_outlier_removal={\"skip\":False, \"sigma\":3, \"kernel\":(1,15)},\n",
    "             laplacianfilter_outlier_removal={\"skip\":False, \"sigma\":50},\n",
    "             second_bckg_subtract={\"skip\":False,\"bckg_rows\":[0,1,2,3,4,5,6,-6,-5,-4,-3,-2,-1]},\n",
    "             track_source_location={\"skip\":False,\"reject_disper\":False,\"reject_spatial\":True}\n",
    "            ):\n",
    "    '''\n",
    "    Performs custom Stage 3 calibrations on all *_calints.fits files located in the filesdir.\n",
    "    Can be run on *.fits files that have already been run on this step, if you want only to\n",
    "    load the data from those *.fits files.\n",
    "    \n",
    "    :param filesdir: str. Directory where the *_calints.fits files you want to calibrate are stored.\n",
    "    :param outdir: str. Directory where you want the additionally-calibrated .fits files to be stored,\n",
    "                   as well as any output images for reference.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :param frames_to_reject: list of int. Indices of frames that you want to reject, for reasons like\n",
    "                             a high-gain antenna move, a satellite crossing, or an anomalous drop/rise in flux.\n",
    "    :param loss_stats: dict.\n",
    "    :param mask_flagged_pixels: dict.\n",
    "    :param iteration_outlier_removal: dict.\n",
    "    :param spatialfilter_outlier_removal: dict.\n",
    "    :param laplacianfilter_outlier_removal: dict.\n",
    "    :param second_bckg_subtract: dict.\n",
    "    :param track_source_location: dict.\n",
    "    :return: calibrated .fits files in the outdir.\n",
    "    '''\n",
    "    # Create the output directory if it does not yet exist.\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    if not os.path.exists(os.path.join(outdir, \"output_imgs_calibration\")):\n",
    "        os.makedirs(os.path.join(outdir, \"output_imgs_calibration\"))\n",
    "    \n",
    "    files = sorted(glob.glob(os.path.join(filesdir,'*_calints.fits')))\n",
    "    print(\"Performing Stage 3 calibrations on the following files:\")\n",
    "    t0 = time.time()\n",
    "    for file in files:\n",
    "        with fits.open(file) as file:\n",
    "            print(file[0].header[\"FILENAME\"])\n",
    "    \n",
    "    # First, we need to stitch all the segments together. This step is not optional.\n",
    "    # It can, however, be run on additionally-calibrated files if you point it to the\n",
    "    # directory where those are held.\n",
    "    segments, errors, segstarts, wavelengths, dqflags, times = stitch_files(files)\n",
    "    \n",
    "    print(\"Read all files and collected needed data.\")\n",
    "    print(\"Creating the aperture for extraction of the data and saving an image of it for reference...\")\n",
    "    aperture = np.ones(np.shape(segments))\n",
    "    aperture[:,\n",
    "             trace_aperture[\"hcut1\"]:trace_aperture[\"hcut2\"],\n",
    "             trace_aperture[\"vcut1\"]:trace_aperture[\"vcut2\"]] = 0\n",
    "    fig, ax, im = img(aperture[0, :, :], aspect=5)\n",
    "    plt.savefig(os.path.join(outdir, \"output_imgs_calibration/trace_aperture.pdf\"), dpi=300)\n",
    "    plt.close(fig)\n",
    "    fig, ax, im = img(segments[0,\n",
    "                               trace_aperture[\"hcut1\"]:trace_aperture[\"hcut2\"],\n",
    "                               trace_aperture[\"vcut1\"]:trace_aperture[\"vcut2\"]],\n",
    "                      aspect=5)\n",
    "    plt.savefig(os.path.join(outdir, \"output_imgs_calibration/trace_extracted_region.pdf\"), dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    plt.imshow(np.log10(np.abs(segments[0,:,:])))\n",
    "    plt.title(\"Frame 0 log10 abs before Stage 3 corrections.\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    if not loss_stats_step[\"skip\"]:\n",
    "        num_pixels_lost_to_DQ_flags = loss_stats(dqflags, trace_aperture, outdir)\n",
    "        print(\"%.0f pixels were marked by DQ flags.\" % num_pixels_lost_to_DQ_flags)\n",
    "    \n",
    "    if not mask_flagged_pixels[\"skip\"]:\n",
    "        segments, num_pixels_lost_by_flagging = mask_flagged(segments, dqflags, trace_aperture)\n",
    "    else:\n",
    "        num_pixels_lost_by_flagging = 0\n",
    "    \n",
    "    if not iteration_outlier_removal[\"skip\"]:\n",
    "        segments, num_pixels_lost_by_iteration = iterate_outlier_removal(segments, dqflags, trace_aperture,\n",
    "                                                                         n=iteration_outlier_removal[\"n\"],\n",
    "                                                                         sigma=iteration_outlier_removal[\"sigma\"])\n",
    "    else:\n",
    "        num_pixels_lost_by_iteration = 0\n",
    "    \n",
    "    if not spatialfilter_outlier_removal[\"skip\"]:\n",
    "        segments, num_pixels_lost_by_filtering = spatial_outlier_removal(segments, trace_aperture,\n",
    "                                                                         sigma=spatialfilter_outlier_removal[\"sigma\"],\n",
    "                                                                         kernel=spatialfilter_outlier_removal[\"kernel\"])\n",
    "    \n",
    "    else:\n",
    "        num_pixels_lost_by_filtering = 0\n",
    "        \n",
    "    if not laplacianfilter_outlier_removal[\"skip\"]:\n",
    "        segments, num_pixels_lost_by_laplacian = laplacian_outlier_removal(segments, errors, trace_aperture,\n",
    "                                                                           sigma=laplacianfilter_outlier_removal[\"sigma\"],\n",
    "                                                                           verbose=True)\n",
    "    else:\n",
    "        num_pixels_lost_by_laplacian = 0\n",
    "    \n",
    "    if not second_bckg_subtract[\"skip\"]:\n",
    "        segments = bckg_subtract(segments,\n",
    "                                 bckg_rows=second_bckg_subtract[\"bckg_rows\"])\n",
    "    \n",
    "    plt.imshow(np.log10(np.abs(segments[0,:,:])))\n",
    "    plt.title(\"Frame 0 log10 abs after Stage 3 corrections.\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    if not track_source_location[\"skip\"]:\n",
    "        frames_rejected_by_source_motion = gaussian_source_track(segments,\n",
    "                                                                 reject_dispersion_direction=track_source_location[\"reject_disper\"],\n",
    "                                                                 reject_spatial_direction=track_source_location[\"reject_spatial\"])\n",
    "        for frame in frames_rejected_by_source_motion:\n",
    "            if frame not in frames_to_reject:\n",
    "                frames_to_reject.append(frame)\n",
    "    \n",
    "    totalmasked = num_pixels_lost_by_flagging + num_pixels_lost_by_iteration + num_pixels_lost_by_filtering + num_pixels_lost_by_laplacian\n",
    "    total = np.shape(segments)[0]*(trace_aperture[\"vcut2\"]-trace_aperture[\"vcut1\"])*(trace_aperture[\"hcut2\"]-trace_aperture[\"hcut1\"])\n",
    "    print(\"In all, %.0f trace pixels out of %.0f were masked (fraction of %.2f) and %.0f frames will be skipped.\" % (totalmasked, total, totalmasked/total, len(frames_to_reject)))\n",
    "    \n",
    "    print(\"Stage 3 calibrations completed in %.3f minutes.\" % ((time.time()-t0)/60))\n",
    "    \n",
    "    print(\"Writing calibrated fits file as several .fits file...\")\n",
    "    for i, file in enumerate(files):\n",
    "        outfile = os.path.join(outdir, \"postprocessed_{0:g}.fits\".format(i))\n",
    "        shutil.copy(file,outfile)\n",
    "        with fits.open(outfile, mode=\"update\") as fits_file:\n",
    "            # Need to write new objects \"segstarts\" and \"frames_to_reject\" to fits file,\n",
    "            # and update data, int_times, and wavelength attributes to be concatenated arrays.\n",
    "\n",
    "            # Create REJECT ImageHDU to contain frames_to_reject object.\n",
    "            REJECT = fits.ImageHDU(np.array(frames_to_reject), name='REJECT')\n",
    "            # Append REJECT to the hdulist of fits_file.\n",
    "            fits_file.append(REJECT)\n",
    "\n",
    "            # Create SEGSTARTS ImageHDU to contain segstarts object.\n",
    "            SEGSTARTS = fits.ImageHDU(np.array(segstarts), name='SEGSTARTS')\n",
    "            # Append SEGSTARTS to the hdulist of fits_file.\n",
    "            fits_file.append(SEGSTARTS)\n",
    "\n",
    "            # Write calibrated image data, times, and wavelengths to new fits file.\n",
    "            if i == 0:\n",
    "                fits_file['SCI'].data = segments[:segstarts[i],:,:]\n",
    "            else:\n",
    "                fits_file['SCI'].data = segments[segstarts[i-1]:segstarts[i],:,:]\n",
    "\n",
    "            # All modified headers get written out.\n",
    "            fits_file.writeto(outfile, overwrite=True)\n",
    "    print(\"Wrote calibrated postprocessed_#.fits files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_files(files):\n",
    "    '''\n",
    "    Reads all *_calints.fits files given by the filepaths provided\n",
    "    and stitches them together.\n",
    "    '''\n",
    "    if \"postprocessed\" in files[0]:\n",
    "        print(\"Reading post-processed files, adjusting outputs accordingly...\")\n",
    "        # Reading a postprocessed file, adjust strategy!\n",
    "        for i, file in enumerate(files):\n",
    "            print(\"Attempting to locate file: \" + file)\n",
    "            fitsfile = dm.open(file)\n",
    "            print(\"Loaded the file \" + file + \" successfully.\")\n",
    "\n",
    "            # Need to retrieve the wavelength object, science object, and DQ object from this.\n",
    "            if i == 0:\n",
    "                segments = fitsfile.data\n",
    "                errors = fitsfile.err\n",
    "                segstarts = [np.shape(fitsfile.data)[0]]\n",
    "                wavelengths = [fitsfile.wavelength]\n",
    "                dqflags = fitsfile.dq\n",
    "                times = fitsfile.int_times[\"int_mid_MJD_UTC\"]\n",
    "            else:\n",
    "                segments = np.concatenate([segments, fitsfile.data], 0)\n",
    "                errors = np.concatenate([errors, fitsfile.err], 0)\n",
    "                segstarts.append(np.shape(fitsfile.data)[0] + sum(segstarts))\n",
    "                wavelengths.append(fitsfile.wavelength)\n",
    "                dqflags = np.concatenate([dqflags, fitsfile.dq], 0)\n",
    "                times = np.concatenate([times, fitsfile.int_times[\"int_mid_MJD_UTC\"]], 0)\n",
    "            \n",
    "            fitsfile.close()\n",
    "            \n",
    "            with fits.open(file) as f:\n",
    "                frames_to_reject = f[\"REJECT\"].data\n",
    "            \n",
    "            print(\"Retrieved segments, wavelengths, DQ flags, and times from file: \" + file)\n",
    "            print(\"Closing file and moving on to next one...\")\n",
    "        return segments, errors, segstarts, wavelengths, dqflags, times, frames_to_reject\n",
    "    else:\n",
    "        for i, file in enumerate(files):\n",
    "            print(\"Attempting to locate file: \" + file)\n",
    "            fitsfile = dm.open(file)\n",
    "            print(\"Loaded the file \" + file + \" successfully.\")\n",
    "\n",
    "            # Need to retrieve the wavelength object, science object, and DQ object from this.\n",
    "            if i == 0:\n",
    "                segments = fitsfile.data\n",
    "                errors = fitsfile.err\n",
    "                segstarts = [np.shape(fitsfile.data)[0]]\n",
    "                wavelengths = [fitsfile.wavelength]\n",
    "                dqflags = fitsfile.dq\n",
    "                times = fitsfile.int_times[\"int_mid_MJD_UTC\"]\n",
    "            else:\n",
    "                segments = np.concatenate([segments, fitsfile.data], 0)\n",
    "                errors = np.concatenate([errors, fitsfile.err], 0)\n",
    "                segstarts.append(np.shape(fitsfile.data)[0] + sum(segstarts))\n",
    "                wavelengths.append(fitsfile.wavelength)\n",
    "                dqflags = np.concatenate([dqflags, fitsfile.dq], 0)\n",
    "                times = np.concatenate([times, fitsfile.int_times[\"int_mid_MJD_UTC\"]], 0)\n",
    "\n",
    "            print(\"Retrieved segments, wavelengths, DQ flags, and times from file: \" + file)\n",
    "            print(\"Closing file and moving on to next one...\")\n",
    "            fitsfile.close()\n",
    "        return segments, errors, segstarts, wavelengths, dqflags, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb072906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_stats(dqflags, trace_aperture, outdir):\n",
    "    '''\n",
    "    Gets pixel loss statistics for the region inside of the trace.\n",
    "    \n",
    "    :param dqflags: 3D array. Integrations x rows x cols of data quality flags.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :return: int of how many pixels were affected by DQ flags.\n",
    "    '''\n",
    "    # Create the aperture for counting loss statistics.\n",
    "    aperture = np.ones(np.shape(dqflags))\n",
    "    aperture[:,\n",
    "             trace_aperture[\"hcut1\"]:trace_aperture[\"hcut2\"],\n",
    "             trace_aperture[\"vcut1\"]:trace_aperture[\"vcut2\"]] = 0\n",
    "    \n",
    "    print(\"Checking to see how much of the trace data will be lost to dq flags...\")\n",
    "    loss_stats = []\n",
    "    for k in range(np.shape(dqflags)[0]):\n",
    "        dqflag_arr = dqflags[k,\n",
    "                             trace_aperture[\"hcut1\"]:trace_aperture[\"hcut2\"],\n",
    "                             trace_aperture[\"vcut1\"]:trace_aperture[\"vcut2\"]]\n",
    "        loss_stats.append(np.count_nonzero(dqflag_arr))\n",
    "    \n",
    "    # Report what unique flags appear in this dataset.\n",
    "    unique = []\n",
    "    \n",
    "    for row in np.ma.masked_array(dqflags[k, :, :], aperture[k, :, :]):\n",
    "        for element in row:\n",
    "            if element not in unique:\n",
    "                unique.append(element)\n",
    "    del(unique[0])\n",
    "    print(\"The following flags were reported:\")\n",
    "    print(unique)\n",
    "    \n",
    "    # Creates an image of the aperture with flags.\n",
    "    flagged_arr = np.where(np.ma.masked_array(dqflags[0, :, :], aperture[0, :, :]) > 0, 1, 0)\n",
    "    fig, ax, im = img(np.ma.masked_array(flagged_arr, aperture[0, :, :]), aspect=5)\n",
    "    plt.savefig(os.path.join(outdir, \"output_imgs_calibration/trace_aperture_with_flags.pdf\"), dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Reports pixel loss stats.\n",
    "    print(\"The total number of lost trace pixels is %.3f.\" % np.sum(loss_stats))\n",
    "    print(\"The total percentage of pixels being lost is %.3f.\" % (100*(np.sum(loss_stats)/(np.shape(dqflags)[0]*(trace_aperture[\"vcut2\"]-trace_aperture[\"vcut1\"])*(trace_aperture[\"hcut2\"]-trace_aperture[\"hcut1\"])))))\n",
    "    print(\"The median number of lost trace pixels is %.3f.\" % np.median(loss_stats))\n",
    "    print(\"The median percentage of pixels being lost is %.3f.\" % (100*(np.median(loss_stats)/((trace_aperture[\"vcut2\"]-trace_aperture[\"vcut1\"])*(trace_aperture[\"hcut2\"]-trace_aperture[\"hcut1\"])))))\n",
    "    print(\"Creating a histogram showing pixel loss statistics...\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.hist(loss_stats, density=True, bins=70)\n",
    "    ax.set_xlabel('number of flagged trace pixels')\n",
    "    ax.set_ylabel('frequency')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return np.sum(loss_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_flagged(segments, dqflags, trace_aperture):\n",
    "    '''\n",
    "    Mask all pixels flagged by the data quality array with their medians in time.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param dqflags: 3D array. Integrations x rows x cols of data quality flags.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :return: segments array with flagged pixels masked, and int of how many pixels in the trace were affected\n",
    "             by this process.\n",
    "    '''\n",
    "    # Get borders to evaluate.\n",
    "    hcut1, hcut2, vcut1, vcut2 = trace_aperture[\"hcut1\"], trace_aperture[\"hcut2\"], trace_aperture[\"vcut1\"], trace_aperture[\"vcut2\"]\n",
    "    \n",
    "    # Turn dqflags into mask arrays.\n",
    "    dq_mask = np.empty_like(dqflags)\n",
    "    dq_mask[:, :, :] = np.where(dqflags[:, :, :] > 0, 1, 0)\n",
    "    \n",
    "    print(\"Masking flagged pixels inside and outside of the trace...\")\n",
    "    t0 = time.time()\n",
    "    masked_flagged = 0\n",
    "    \n",
    "    for i in range(np.shape(segments)[1]):\n",
    "        for j in range(np.shape(segments)[2]):\n",
    "            # Track temporal variations and replace any flagged pixels with the temporal median\n",
    "            # that was calculated out of only unmasked values.\n",
    "            pmed = np.ma.median(np.ma.masked_array(segments[:, i, j], dq_mask[:, i, j]))\n",
    "            psigma = np.std(np.ma.masked_array(segments[:, i, j], dq_mask[:, i, j]))\n",
    "            if (i in range(hcut1, hcut2) and j in range(vcut1, vcut2)):\n",
    "                masked_flagged += np.count_nonzero(dqflags[:, i, j])\n",
    "            segments[:, i, j] = np.where(dq_mask[:, i, j] == 1, pmed, segments[:, i, j])\n",
    "    print(\"Masked %.0f flagged trace pixels in %.3f seconds.\" % (masked_flagged, time.time() - t0))\n",
    "    return segments, masked_flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_outlier_removal(segments, dqflags, trace_aperture, n, sigma):\n",
    "    '''\n",
    "    Iterate and remove outliers to reject CRs. Does not use masked values to compute the time median.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param dqflags: 3D array. Integrations x rows x cols of data quality flags.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :param n: int. Number of times to iterate.\n",
    "    :param sigma: float. Sigma at which to reject outliers.\n",
    "    :return: segments array with CRs rejected, and int of how many pixels in the trace were affected\n",
    "             by this process.\n",
    "    '''\n",
    "    # Get borders to evaluate.\n",
    "    hcut1, hcut2, vcut1, vcut2 = trace_aperture[\"hcut1\"], trace_aperture[\"hcut2\"], trace_aperture[\"vcut1\"], trace_aperture[\"vcut2\"]\n",
    "    \n",
    "    # Turn dqflags into mask arrays.\n",
    "    dq_mask = np.empty_like(dqflags)\n",
    "    dq_mask[:, :, :] = np.where(dqflags[:, :, :] > 0, 1, 0)\n",
    "    \n",
    "    print(\"Masking pixels with substantial time variations...\")\n",
    "    t0 = time.time()\n",
    "    masked_iter = 0\n",
    "    \n",
    "    for iteration in range(n):\n",
    "        print(\"On iteration %.0f...\" % iteration)\n",
    "        t02 = time.time()\n",
    "        for i in range(np.shape(segments)[1]):\n",
    "            for j in range(np.shape(segments)[2]):\n",
    "                # Track temporal variation of a single pixel and mask anywhere that pixel is 10sigma\n",
    "                # deviating from its usual levels. This should help suppress noise.\n",
    "                pmed = np.ma.median(np.ma.masked_array(segments[:, i, j], dq_mask[:, i, j]))\n",
    "                psigma = np.std(np.ma.masked_array(segments[:, i, j], dq_mask[:, i, j]))\n",
    "                if (i in range(hcut1, hcut2) and j in range(vcut1, vcut2)):\n",
    "                    maskcount = np.where(np.abs(segments[:, i, j] - pmed)>sigma*psigma, 1, 0)\n",
    "                    masked_iter += np.count_nonzero(maskcount)\n",
    "                segments[:, i, j] = np.where(np.abs(segments[:, i, j] - pmed)>sigma*psigma, pmed, segments[:, i, j])\n",
    "        print(\"Performed round %.0g of %.2f-sigma temporal outlier rejection in %.3f seconds.\" % (iteration, sigma, time.time()-t02))\n",
    "    print(\"Masked %.0f trace pixels for significant temporal variations in %.3f seconds.\" % (masked_iter, time.time()-t0))\n",
    "    return segments, masked_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69526952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_outlier_removal(segments, trace_aperture, sigma, kernel):\n",
    "    '''\n",
    "    Median filter the image to remove hot pixels.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :param sigma: float. Sigma at which to reject outliers.\n",
    "    :param kernel: tuple of odd ints. Kernel to use for spatial filtering.\n",
    "    '''\n",
    "    # Get borders to evaluate.\n",
    "    hcut1, hcut2, vcut1, vcut2 = trace_aperture[\"hcut1\"], trace_aperture[\"hcut2\"], trace_aperture[\"vcut1\"], trace_aperture[\"vcut2\"]\n",
    "    \n",
    "    print(\"Performing hot pixel masking through spatial median filtering...\")\n",
    "    masked_filter = 0\n",
    "    cleaned_segments = np.zeros_like(segments)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for i in range(np.shape(segments)[0]):\n",
    "        # Clean the array.\n",
    "        cleaned_segments[i, :, :] = clean(segments[i, :, :], sigma, kernel)\n",
    "        \n",
    "        # Check where it has been changed.\n",
    "        maskcount = np.empty((hcut2-hcut1, vcut2-vcut1))\n",
    "        maskcount = np.where(cleaned_segments[i, hcut1:hcut2, vcut1:vcut2] != segments[i, hcut1:hcut2, vcut1:vcut2], 1, 0)\n",
    "        masked_filter += np.count_nonzero(maskcount)\n",
    "    print(\"Masked %.0f trace pixels for significant spatial variation in %.3f seconds.\" % (masked_filter, time.time()-t0))\n",
    "    print(\"Performed median filtering in %.3f seconds.\" % (time.time()-t0))\n",
    "    return cleaned_segments, masked_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_outlier_removal(segments, errors, trace_aperture, sigma=50, verbose=False):\n",
    "    '''\n",
    "    Convolves a Laplacian kernel with the segments array to replace spatial outliers with\n",
    "    the median of the surrounding 3x3 kernel.\n",
    "    \n",
    "    :param segments: 3D array. The segments(t,x,y) array from which outliers will be removed.\n",
    "    :param sigma: float. Threshold of deviation from median of Laplacian image, above which a pixel\n",
    "                  will be flagged as an outlier and masked.\n",
    "    :param verbose: bool. If True, occasionally prints out a progress report.\n",
    "    :return: segments(t,x,y) array with spatial outliers masked.\n",
    "    '''\n",
    "    # Get borders to evaluate.\n",
    "    hcut1, hcut2, vcut1, vcut2 = trace_aperture[\"hcut1\"], trace_aperture[\"hcut2\"], trace_aperture[\"vcut1\"], trace_aperture[\"vcut2\"]\n",
    "    \n",
    "    l = 0.25*np.array([[0,-1,0],[-1,4,-1],[0,-1,0]])\n",
    "    segmentsc = deepcopy(segments)\n",
    "    errorsc = deepcopy(errors)\n",
    "    \n",
    "    bad_pix_removed = 0\n",
    "    t0 = time.time()\n",
    "    steps_taken = 0\n",
    "    nsteps = np.shape(segments)[0]\n",
    "    \n",
    "    print(\"Cleaning %.1f-sigma outliers with Laplacian edge detection...\" % sigma)\n",
    "    for k in range(np.shape(segments)[0]):\n",
    "        # Iterate over frames.\n",
    "        print(\"On frame %.0f...\" % k)\n",
    "        # Estimate readnoise.\n",
    "        errf  = errorsc[k,:,:]**2\n",
    "        errf -= segmentsc[k,:,:] # remove shot noise from errors to get read noise variance.\n",
    "        errf[errf < 0] = 0 # enforce positivity.\n",
    "        errf  = np.sqrt(errf) # turn variance into readnoise.\n",
    "        rn    = np.mean(errf) # mean readnoise is our estimate.\n",
    "        \n",
    "        if (verbose and k == 0):\n",
    "            print(\"Estimated readnoise: %.10f\" % rn)\n",
    "        \n",
    "        # Build noise model.\n",
    "        NOISE = np.sqrt(median_filter(np.abs(segmentsc[k,:,:]),size=5)+rn**2)\n",
    "        NOISE[NOISE <= 0] = np.min(NOISE[np.nonzero(NOISE)]) # really want to avoid nans\n",
    "        if (verbose and k == 0):\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.imshow(np.log10(NOISE))\n",
    "            plt.title(\"Noise model for LED\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        original_shape = np.shape(segments[k,:,:])\n",
    "        ss_shape = (original_shape[0]*2,original_shape[1]*2) # double subsampling\n",
    "        subsample = np.empty(ss_shape)\n",
    "        \n",
    "        # Subsample the array.\n",
    "        for i in range(ss_shape[0]):\n",
    "            for j in range(ss_shape[1]):\n",
    "                try:\n",
    "                    subsample[i,j] = segments[int((i+1)/2),int((j+1)/2),k]\n",
    "                except IndexError:\n",
    "                    subsample[i,j] = 0\n",
    "        \n",
    "        # Convolve subsample with laplacian.\n",
    "        lap_img = np.convolve(l.flatten(),subsample.flatten(),mode='same').reshape(ss_shape)\n",
    "        lap_img[lap_img < 0] = 0 # force positivity\n",
    "        \n",
    "        # Resample to original size.\n",
    "        resample = np.empty(original_shape)\n",
    "        for i in range(original_shape[0]):\n",
    "            for j in range(original_shape[1]):\n",
    "                resample[i,j] = 0.25*(lap_img[2*i-1,2*j-1] +\n",
    "                                      lap_img[2*i-1,2*j] +\n",
    "                                      lap_img[2*i,2*j-1] +\n",
    "                                      lap_img[2*i,2*j])\n",
    "                \n",
    "        # Divide by subsample factor times noise model.\n",
    "        scaled_resample = resample/(2*NOISE)\n",
    "        \n",
    "        # Spot outliers.\n",
    "        med = np.median(scaled_resample)\n",
    "        if (verbose and k == 0):\n",
    "            print(\"Median of scaled resampled laplacian image: %.10f\" % med)\n",
    "        scaled_resample[np.abs(scaled_resample-med) < med*sigma] = 0 # any not zero after this are rays.\n",
    "        scaled_resample[scaled_resample!=0] = 1 # for visualization\n",
    "        \n",
    "        if (verbose and k == 0):\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.imshow(scaled_resample)\n",
    "            plt.title(\"Where CRs and hot pixels were detected\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        # Correct frames\n",
    "        for i, j in zip(np.where(scaled_resample!=0)[0],np.where(scaled_resample!=0)[1]):\n",
    "            segments[k,i,j] = np.median(segments[k,i-1:i+2,j-1:j+2]) # replace with local median\n",
    "            if (hcut1 <= i <= hcut2 and vcut1 <= j <= vcut2):\n",
    "                bad_pix_removed += 1\n",
    "            \n",
    "        # Report progress.\n",
    "        steps_taken += 1\n",
    "        if (steps_taken % int(nsteps*.1) == 0 and verbose):\n",
    "            iter_rate = steps_taken/(time.time()-t0)\n",
    "            print(\"%.0f-percent done. Time elapsed: %.0f seconds. Estimated time remaining: %.0f seconds.\" % (steps_taken*100/nsteps, time.time()-t0, (nsteps-steps_taken)/iter_rate))\n",
    "    print(\"Iterations complete. Removed %.0f spatial outliers in %.0f seconds.\" % (bad_pix_removed, time.time()-t0))\n",
    "    return segments, bad_pix_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba629c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bckg_subtract(segments, bckg_rows):\n",
    "    '''\n",
    "    Subtract background signal using the rows defined by bckg_rows as the background.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param bckg_rows: list of integers. Indices of the rows to use as the background region.\n",
    "    :return: segments with background subtracted.\n",
    "    '''\n",
    "    print(\"Performing additional background subtraction...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in range(np.shape(segments)[0]):\n",
    "        background_region = segments[i, bckg_rows, :]\n",
    "        background_region = sigma_clip(background_region, sigma=3)\n",
    "        mmed = np.ma.median(background_region)\n",
    "        background_region = background_region.filled(fill_value=mmed)\n",
    "        background = background_region.mean(axis=0)\n",
    "        background = np.array([background,]*np.shape(segments)[1])\n",
    "        segments[i, :, :] = segments[i, :, :] - background\n",
    "        if (i%1000 == 0 and i != 0):\n",
    "            # Report progress every 1,000 integrations.\n",
    "            elapsed_time = time.time()-t0\n",
    "            iterrate = i/elapsed_time\n",
    "            iterremain = np.shape(segments)[0] - i\n",
    "            print(\"On integration %.0f. Elapsed time is %.3f seconds.\" % (i, elapsed_time))\n",
    "            print(\"Average rate of integration processing: %.3f ints/s.\" % iterrate)\n",
    "            print(\"Estimated time remaining: %.3f seconds.\\n\" % (iterremain/iterrate))\n",
    "    print(\"Additional background subtraction completed in %.3f seconds.\" % (time.time()-t0))\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c255e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_source_track(segments, reject_dispersion_direction=True, reject_spatial_direction=False):\n",
    "    '''\n",
    "    Tracks the location of the trace between frames and reports frame numbers that show\n",
    "    significant deviations from the usual location.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param reject_dispersion_direction: bool. Whether to reject outliers of position in the dispersion direction.\n",
    "    :param reject_spatial_direction: bool. Whether to reject outliers of position in the spatial direction.\n",
    "    :return: reject_frames list of ints showing which frames are to get rejected by\n",
    "    '''\n",
    "    reject_frames = []\n",
    "    t0 = time.time()\n",
    "    source_pos_disp = []\n",
    "    source_pos_cros = []\n",
    "    print(\"Fitting source position for each integration...\")\n",
    "    for k in range(np.shape(segments)[0]):\n",
    "        # First find the dispersion axis position.\n",
    "        profile = np.sum(segments[k, :, :], axis=0)\n",
    "        profile = profile/np.max(profile) # normalize amplitude to 1 for ease of fit\n",
    "        fitter = modeling.fitting.LevMarLSQFitter()\n",
    "        model = modeling.models.Gaussian1D(amplitude=1, mean=100, stddev=1)\n",
    "        fitted_model = fitter(model, [i for i in range(np.shape(profile)[0])], profile)\n",
    "        source_pos_disp.append(fitted_model.mean[0])\n",
    "\n",
    "        # Then find the cross dispersion axis position.\n",
    "        profile = np.sum(segments[k, :, :], axis=1)\n",
    "        profile = profile/np.max(profile) # normalize amplitude to 1 for ease of fit\n",
    "        fitter = modeling.fitting.LevMarLSQFitter()\n",
    "        model = modeling.models.Gaussian1D(amplitude=1, mean=14, stddev=1)\n",
    "        fitted_model = fitter(model, [i for i in range(np.shape(profile)[0])], profile)\n",
    "        source_pos_cros.append(fitted_model.mean[0])\n",
    "\n",
    "        if (k%500 == 0 and k != 0):\n",
    "            # Report progress every 500 integrations.\n",
    "            elapsed_time = time.time()-t0\n",
    "            iterrate = k/elapsed_time\n",
    "            iterremain = np.shape(segments)[0] - k\n",
    "            print(\"On integration %.0f. Elapsed time is %.3f seconds.\" % (k, elapsed_time))\n",
    "            print(\"Average rate of integration processing: %.3f ints/s.\" % iterrate)\n",
    "            print(\"Estimated time remaining: %.3f seconds.\\n\" % (iterremain/iterrate))\n",
    "    print(\"Fit source positions in %.3f minutes.\" % ((time.time()-t0)/60))\n",
    "    \n",
    "    # Now that we have the positions, build list of integration indices to reject for being too far off.\n",
    "    mpd = np.median(source_pos_disp)\n",
    "    spd = np.std(source_pos_disp)\n",
    "    mpc = np.median(source_pos_cros)\n",
    "    spc = np.std(source_pos_cros)\n",
    "\n",
    "    print(\"Median position: \" + str(mpd) + \", \" + str(mpc))\n",
    "    print(\"Sigma: \" + str(spd) + \", \" + str(spc))\n",
    "    skipped = 0\n",
    "    for k in range(np.shape(segments)[0]):\n",
    "        if reject_dispersion_direction:\n",
    "            if np.abs(mpd - source_pos_disp[k]) > 3*spd:\n",
    "                reject_frames.append(k)\n",
    "                skipped += 1\n",
    "        \n",
    "        if reject_spatial_direction:\n",
    "            if np.abs(mpc - source_pos_cros[k]) > 3*spc:\n",
    "                reject_frames.append(k)\n",
    "                skipped += 1\n",
    "    print(\"%.0f integrations had source positions significantly off from the median position and will be rejected.\" % skipped)\n",
    "    return reject_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doStage4(filepaths, outdir,\n",
    "             trace_aperture={\"hcut1\":0,\n",
    "                             \"hcut2\":-1,\n",
    "                             \"vcut1\":0,\n",
    "                             \"vcut2\":-1},\n",
    "             extract_light_curves={\"skip\":False,\n",
    "                                   \"wavbins\":np.linspace(0.6,5.3,70),\n",
    "                                   \"ext_type\":\"box\"},\n",
    "             median_normalize_curves={\"skip\":False},\n",
    "             sigma_clip_curves={\"skip\":False,\n",
    "                                \"b\":100,\n",
    "                                \"clip_at\":5},\n",
    "             fix_transit_times={\"skip\":False,\n",
    "                                \"epoch\":None},\n",
    "             plot_light_curves={\"skip\":False},\n",
    "             save_light_curves={\"skip\":False}\n",
    "            ):\n",
    "    '''\n",
    "    Performs Stage 4 extractions on the files located at filepaths.\n",
    "    \n",
    "    :param filepath: lst of str. Location of the postprocessed_*.fits files you want to extract spectra from.\n",
    "    :param outdir: str. Location where you want output images and text files to be saved to.\n",
    "    :param trace_aperture: dict. Keywords are \"hcut1\", \"hcut2\", \"vcut1\", \"vcut2\", all integers\n",
    "                           denoting the rows and columns respectively that define the edges of\n",
    "                           the aperture bounding the trace.\n",
    "    :return: .txt files of curves extracted from the postprocessed_*.fits files.\n",
    "    '''\n",
    "    print(\"Performing Stage 4 extractions of spectra from the data located at: {}\".format(filepaths))\n",
    "    \n",
    "    # Grab the needed info from the file.\n",
    "    segments, errors, segstarts, wavelengths, dqflags, times, frames_to_reject = stitch_files(filepaths)\n",
    "    \n",
    "    # Build the aperture object.\n",
    "    aperture = np.ones(np.shape(segments))\n",
    "    aperture[:,\n",
    "             trace_aperture[\"hcut1\"]:trace_aperture[\"hcut2\"],\n",
    "             trace_aperture[\"vcut1\"]:trace_aperture[\"vcut2\"]] = 0\n",
    "    \n",
    "    # Should not skip extract light curves! Rest of code breaks.\n",
    "    if not extract_light_curves[\"skip\"]:\n",
    "        wlc, slc, times, central_lams = extract_curves(segments, errors, times, aperture, segstarts, wavelengths, frames_to_reject,\n",
    "                                                       wavbins=extract_light_curves[\"wavbins\"],\n",
    "                                                       ext_type=extract_light_curves[\"ext_type\"])\n",
    "        \n",
    "    if not median_normalize_curves[\"skip\"]:\n",
    "        wlc = median_normalize(wlc)\n",
    "        for i, lc in enumerate(slc):\n",
    "            slc[i] = median_normalize(lc)\n",
    "    \n",
    "    if not sigma_clip_curves[\"skip\"]:\n",
    "        wlc = clip_curve(wlc,\n",
    "                         b=sigma_clip_curves[\"b\"],\n",
    "                         clip_at=sigma_clip_curves[\"clip_at\"])\n",
    "        for i, lc in enumerate(slc):\n",
    "            slc[i] = clip_curve(lc,\n",
    "                                b=sigma_clip_curves[\"b\"],\n",
    "                                clip_at=sigma_clip_curves[\"clip_at\"])\n",
    "            \n",
    "    if not fix_transit_times[\"skip\"]:\n",
    "        print(\"Fixing transit timestamps...\")\n",
    "        times=fix_times(times, wlc=wlc,\n",
    "                        epoch=fix_transit_times[\"epoch\"])\n",
    "        print(\"Fixed.\")\n",
    "        \n",
    "    if not plot_light_curves[\"skip\"]:\n",
    "        print(\"Generating output plots of extracted light curves...\")\n",
    "        imgs_outdir = os.path.join(outdir, \"output_imgs_extraction\")\n",
    "        if not os.path.exists(imgs_outdir):\n",
    "            os.makedirs(imgs_outdir)\n",
    "        plot_curve(times, wlc, \"White light curve\", \"wlc\", imgs_outdir)\n",
    "        for lc, central_lam in zip(slc, central_lams):\n",
    "            plot_curve(times, lc, \"Spectroscopic light curve at %.3f micron\" % central_lam, \"slc_%.3fmu\" % central_lam, imgs_outdir)\n",
    "        print(\"Plots generated.\")\n",
    "    \n",
    "    if not save_light_curves[\"skip\"]:\n",
    "        print(\"Writing all curves to .txt files...\")\n",
    "        txts_outdir = os.path.join(outdir, \"output_txts_extraction\")\n",
    "        if not os.path.exists(txts_outdir):\n",
    "            os.makedirs(txts_outdir)\n",
    "        write_light_curve(times, wlc, \"wlc\", txts_outdir)\n",
    "        for i, lc in enumerate(slc):\n",
    "            if extract_light_curves[\"wavbins\"][i] == extract_light_curves[\"wavbins\"][-1]:\n",
    "                pass\n",
    "            else:\n",
    "                write_light_curve(times, lc, \"slc_%.3fmu_%.3fmu\" % (extract_light_curves[\"wavbins\"][i],extract_light_curves[\"wavbins\"][i+1]), txts_outdir)\n",
    "        print(\"Files written.\")\n",
    "    print(\"Stage 4 finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_curves(segments, errors, times, aperture, segstarts, wavelengths, frames_to_reject, wavbins, ext_type=\"box\"):\n",
    "    '''\n",
    "    Extract a white light curve and spectroscopic light curves from the trace.\n",
    "    \n",
    "    :param segments: 3D array. Integrations x rows x cols of data.\n",
    "    :param errors: 3D array. Integrations x rows x cols of uncertainties.\n",
    "    :param times: 1D array. Timestamps of integrations.\n",
    "    :param aperture: 3D array. Mask that defines where the trace is.\n",
    "    :param segstarts: list of ints. Defines where new segment files begin.\n",
    "    :param wavelengths: list of lists of floats. The wavelength solutions for each segment.\n",
    "    :param frames_to_reject: list of ints. Frames that will not be added into the light curve.\n",
    "    :param wavbins: list of floats. The edges defining each spectroscopic light curve. The ith bin\n",
    "                    will count pixels that have wavelength solution wavbins[i] <= wav < wavbins[i+1].\n",
    "    :param ext_type: str. Choices are \"box\" or \"opt\".\n",
    "    :return: corrected timestamps, median-normalized white light curve, and median-normalized spectroscopic light curve.\n",
    "    '''\n",
    "    # Get just the trace that you want to sum over.\n",
    "    trace = np.ma.masked_array(segments, aperture)\n",
    "    \n",
    "    # Initialize 1Dspec objects.\n",
    "    oneDspec = []\n",
    "    central_lams = []\n",
    "    times_with_skips = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    masks_built_yet = 0\n",
    "    if ext_type == \"opt\":\n",
    "        spatial_profile = get_spatial_profile(trace, ext_type=ext_type)\n",
    "    for k in range(np.shape(segments)[0]):\n",
    "        if k in frames_to_reject:\n",
    "            print(\"Integration %.0f will be skipped.\" % k)\n",
    "        else:\n",
    "            # Not a rejected frame, so proceed.\n",
    "            print(\"Gathering 1D spectrum of integration %.0f...\" % k)\n",
    "            times_with_skips.append(times[k])\n",
    "            \n",
    "            # When we are at the start of a new segment, we have to rebuild the wavelength masks.\n",
    "            if (k in segstarts or masks_built_yet == 0):\n",
    "                print(\"Building wavelength masks...\")\n",
    "                masks = []\n",
    "                for i in range(1):\n",
    "                    if (k == segstarts[i] or masks_built_yet == 0):\n",
    "                        wavelength = wavelengths[i]\n",
    "                for j, w in enumerate(wavbins):\n",
    "                    if w == wavbins[-1]:\n",
    "                        # Don't build a bin at the end of the wavelength range.\n",
    "                        pass\n",
    "                    else:\n",
    "                        central_lams.append((wavbins[j]+wavbins[j+1])/2)\n",
    "                        mask_step1 = np.where(wavelength <= wavbins[j+1], wavelength, 0)\n",
    "                        mask_step2 = np.where(mask_step1 >= wavbins[j], mask_step1, 0)\n",
    "                        mask = np.where(mask_step2 != 0, 1, 0)\n",
    "                        # Now we want to invert it, setting all 0s to 1s and vice versa.\n",
    "                        mask = np.where(mask == 1, 0, 1)\n",
    "                        masks.append([mask])\n",
    "                masks_built_yet = 1\n",
    "                print(\"Masks built.\")\n",
    "            \n",
    "            if ext_type == \"opt\":\n",
    "                profile  = spatial_profile[k,:,:]\n",
    "                errors2  = errors[k,:,:]**2\n",
    "                errors2 -= trace[k,:,:]\n",
    "                errors2[errors2<=0] = 10**-8\n",
    "                f = np.sum(trace[k,:,:], axis=0)\n",
    "                V = errors2+np.abs(f[np.newaxis,:]*profile)\n",
    "                trace[k,:,:] = (profile * trace[k,:,:] / V)/np.sum(profile ** 2 / V, axis=0)\n",
    "            \n",
    "            spectrum = []\n",
    "            for mask in masks:\n",
    "                spectrum.append(np.sum(np.ma.masked_array(np.ma.masked_array(np.copy(trace[k, :, :]), mask), aperture[k, :, :])))\n",
    "            oneDspec.append(spectrum)\n",
    "            print(\"Collected spectrum.\")\n",
    "        if (k%1000 == 0 and k != 0):\n",
    "            elapsed_time = time.time()-t0\n",
    "            iterrate = k/elapsed_time\n",
    "            iterremain = np.shape(segments)[0] - k\n",
    "            print(\"On integration %.0f. Elapsed time is %.3f seconds.\" % (k, elapsed_time))\n",
    "            print(\"Average rate of integration processing: %.3f ints/s.\" % iterrate)\n",
    "            print(\"Estimated time remaining: %.3f seconds.\\n\" % (iterremain/iterrate))\n",
    "    print(\"Gathered 1D spectra in %.3f minutes.\" % ((time.time()-t0)/60))\n",
    "    \n",
    "    # We now have the oneDspec object. We're going to sum the oneDspec into a wlc,\n",
    "    # then reorganize the oneDspec into a bunch of spectroscopic light curves.\n",
    "    print(\"Producing wlc from 1D spectra...\")\n",
    "    wlc = []\n",
    "    for spectra in oneDspec:\n",
    "        wlc.append(np.sum(spectra))\n",
    "    print(\"Generated wlc.\")\n",
    "    \n",
    "    slc = []\n",
    "    print(\"Reshaping oneDspec into slc...\")\n",
    "    for i, w in enumerate(wavbins):\n",
    "        if w == wavbins[-1]:\n",
    "            # Didn't build a bin for the last wavelength.\n",
    "            pass\n",
    "        else:\n",
    "            lc = []\n",
    "            for j in range(len(wlc)):\n",
    "                lc.append(oneDspec[j][i])\n",
    "            slc.append(np.array(lc))\n",
    "    # Now each object in slc is a full time series corresponding to just one wavelength bin.\n",
    "    print(\"Reshaped. Returning wlc and slc...\")\n",
    "    \n",
    "    return wlc, slc, times_with_skips, np.round(central_lams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca37f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_profile(segments, ext_type=\"box\"):\n",
    "    '''\n",
    "    Builds a spatial profile for extraction.\n",
    "    If ext_type \"box\", profile is unifom 1s.\n",
    "    If ext_type \"opt\", profile is an optimum profile.\n",
    "    '''\n",
    "    if ext_type == \"box\":\n",
    "        P = np.ones_like(segments)\n",
    "    if ext_type == \"opt\":\n",
    "        P = np.empty_like(segments)\n",
    "        # Iterate through frames.\n",
    "        for k in range(np.shape(P)[0]):\n",
    "            P[k,:,:] = polycol(segments[k,:,0], poly_order=4, threshold=3)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polycol(trace, poly_order=4, threshold=3):\n",
    "    '''\n",
    "    Computes spatial profile fit to the trace using a polynomial of the specified order, fitting along columns.\n",
    "    \n",
    "    :param trace: 2D array. A frame out of the trace[y,x,t] array, form trace[y,x].\n",
    "    :param order: int. Order of polynomial to fit as the profile.\n",
    "    :param threshold: float. Sigma threshold at which to mask polynomial fit outliers.\n",
    "    :return: P[x,y] array. An array profile to use for optimal extraction.\n",
    "    '''\n",
    "    # Initialize P as a list.\n",
    "    P = []\n",
    "    \n",
    "    # Iterate on columns.\n",
    "    for i in range(np.shape(trace)[1]):\n",
    "        col = deepcopy(trace[:,i])\n",
    "        j = 0\n",
    "        while True:\n",
    "            p_coeff = np.polyfit(range(np.shape(col)[0]),col,deg=poly_order)\n",
    "            p_col = np.polyval(p_coeff, range(np.shape(col)[0]))\n",
    "\n",
    "            res = np.array(col-p_col)\n",
    "            dev = np.abs(res)/np.std(res)\n",
    "            max_dev_idx = np.argmax(dev)\n",
    "\n",
    "            j += 1\n",
    "            if (dev[max_dev_idx] > threshold and j < 20):\n",
    "                try:\n",
    "                    col[max_dev_idx] = (col[max_dev_idx-1]+col[max_dev_idx+1])/2\n",
    "                except IndexError:\n",
    "                    col[max_dev_idx] = np.median(p_col)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        P.append(p_col)\n",
    "    P = np.array(P).T\n",
    "    P[P < 0] = 0 # enforce positivity\n",
    "    P /= np.sum(P,axis=0) # normalize on columns\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23179dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_normalize(lc):\n",
    "    '''\n",
    "    The extract_curves function is way too big and doing way too much, so I am going to break its\n",
    "    median normalization functionality into here.\n",
    "    '''\n",
    "    return lc/np.median(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d295a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_times(times, wlc=None, epoch=None):\n",
    "    '''\n",
    "    Fixes times in the times array so that the mid-transit time is 0.\n",
    "    \n",
    "    :param times: 1D array. Times in MJD, not corrected for mid-transit.\n",
    "    :param wlc: 1D array or None. If not None and epoch is None, the epoch is defined\n",
    "                as the time when wlc hits its minimum.\n",
    "    :param epoch: float. If not None, the mid-transit time used to correct the times arrays.\n",
    "    :return: corrected times array.\n",
    "    '''\n",
    "    if epoch is None:\n",
    "        if wlc is not None:\n",
    "            minimum_value = min(wlc)\n",
    "            epoch = t[wlc.index(minimum_value)]\n",
    "        else:\n",
    "            epoch = np.mean(t)\n",
    "        \n",
    "    for i in range(len(times)):\n",
    "        times[i] = times[i] - epoch\n",
    "        \n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85071bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_curve(lc, b, clip_at):\n",
    "    '''\n",
    "    Sigma clip the given light curve.\n",
    "    '''\n",
    "    clipcount = 0\n",
    "    for i in np.arange(0, len(lc)+b, b):\n",
    "        try:\n",
    "            # Sigma-clip this segment of wlc and fill the clipped parts with the median.\n",
    "            smed = np.median(lc[i:i+b])\n",
    "            ssig = np.std(lc[i:i+b])\n",
    "            clipcount += np.count_nonzero(np.where(np.abs(lc[i:i+b]-smed) > clip_at*ssig, 1, 0))\n",
    "            lc[i:i+b] = np.where(np.abs(lc[i:i+b]-smed) > clip_at*ssig, smed, lc[i:i+b])\n",
    "        except IndexError:\n",
    "            if len(lc[i:-1]) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                lc[i:-1] = sigma_clip(lc[i:-1], sigma=clip_at)\n",
    "                lc[i:-1] = lc[i:-1].filled(fill_value=np.ma.median(lc[i:-1]))\n",
    "    print(\"Clipped %.0f values from the given light curve.\" % clipcount)\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(t, lc, title, outfile, outdir):\n",
    "    '''\n",
    "    The extract_curves function is way too big and doing way too much, so I am going to break its\n",
    "    plotting functionality into here.\n",
    "    '''\n",
    "    #print(\"Plotting light curve and saving plot...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.scatter(t, lc, s=3)\n",
    "    ax.set_xlabel(\"time since mid-transit [days]\")\n",
    "    ax.set_ylabel(\"relative flux [no units]\")\n",
    "    ax.set_title(title)\n",
    "    plt.savefig(os.path.join(outdir, outfile + \".pdf\"), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ff91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_light_curve(t, lc, outfile, outdir):\n",
    "    '''\n",
    "    Writes light curve to .txt file for future reading.\n",
    "    '''\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    with open('{}/{}.txt'.format(outdir,outfile), mode='w') as file:\n",
    "        file.write(\"#time[MJD]     flux[normalized]\\n\")\n",
    "        for ti, lci in zip(t, lc):\n",
    "            file.write('{:8}   {:8}\\n'.format(ti, lci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doStage5(curvesdir, outdir, exoplanet_params, systematics, spectral_range,\n",
    "             do_fit={\"WLC_LM\":True,\n",
    "                     \"WLC_MCMC\":True,\n",
    "                     \"spec_LM\":True,\n",
    "                     \"spec_MCMC\":True,},\n",
    "             limb_darkening_model={\"model_type\":\"quadratic\",\n",
    "                                   \"stellar_params\":None,\n",
    "                                   \"initial_guess\":[0.1,0.1],},\n",
    "             fixed_param={\"LD_coeffs\":False,\n",
    "                          \"t0\":False,\n",
    "                          \"period\":False,\n",
    "                          \"aoR\":False,\n",
    "                          \"impact\":False,#\"inc\":False,\n",
    "                          \"ecc\":False,\n",
    "                          \"lop\":False,\n",
    "                          \"offset\":False},\n",
    "             MCMC_depth_type=\"standard\",\n",
    "             priors_dict={\"t0\":[-0.1, 0.1],\n",
    "                         \"period\":[0, 100],\n",
    "                         \"aoR\":[0.00001, 10],\n",
    "                         \"impact\":[0,100],#\"inc\":[80,90],\n",
    "                         \"ecc\":[0, 1],\n",
    "                         \"lop\":[0, 90],\n",
    "                         \"offset\":[-0.5, 0.5]},\n",
    "             priors_type=\"uniform\",\n",
    "             reject_threshold=3,\n",
    "             raise_alarm=10,\n",
    "             exoticLD={\"available\":False,\n",
    "                       \"ld_data_path\":None,\n",
    "                       \"ld_grid\":'kurucz',\n",
    "                       \"ld_interpolate_type\":'trilinear'},\n",
    "             save_plots={\"WLC_LM\":True,\n",
    "                         \"WLC_MCMC\":True,\n",
    "                         \"spec_LM\":True,\n",
    "                         \"spec_MCMC\":True}):\n",
    "    '''\n",
    "    Performs Stage 5 LM and MCMC fitting of the light curves in the specified directory.\n",
    "    \n",
    "    :param curvesdir: str. Where the .txt files of the curves you want to analyze are stored.\n",
    "    :param exoplanet_params: dict of float. Contains keywords \"t0\", \"period\", \"rp\", \"aoR\", \"impact\", \"ecc\", \"lop\".\n",
    "    :param systematics: tuple of float. Contains parameters for a linear-in-time fit a+b*(x-0.5).\n",
    "    :param spectral_range: tuple of float. Spectral range being covered.\n",
    "    :param R: float. Defines spectral resolving power.\n",
    "    :param limb_darkening_model: dict. Contains \"model_type\" str which defines model choice (e.g. quadratic, 4-param),\n",
    "                                 \"stellar_params\" tuple of (M_H, Teff, logg) or None if not using, \"initial_guess\"\n",
    "                                 keyword containing tuple of floats.\n",
    "    :param fixed_param. dict of bools. Keywords are parameters that can be held fixed or opened for fitting.\n",
    "                        If True, parameter will be held fixed. If False, parameter is allowed to be fitted.\n",
    "    :param MCMC_depth_type: str. Options are \"standard\" or \"ldcta\".\n",
    "    :param exoticLD: dict. Contains \"available\" bool for whether EXoTiC-LD is on this system,\n",
    "                     \"ld_data_path\" str of where the exotic_ld_data directory is located.\n",
    "    :return: parameters from LM and MCMC fits, transit depths, and errors on depths.\n",
    "    '''\n",
    "    original_systematics = deepcopy(systematics)\n",
    "    # Read out wlc first.\n",
    "    wlc_path = os.path.join(curvesdir, \"wlc.txt\")\n",
    "    wlc, times = read_light_curve(wlc_path)\n",
    "    \n",
    "    # Perform LM fit of wlc.\n",
    "    original_exoplanet_params = deepcopy(exoplanet_params)\n",
    "    if do_fit[\"WLC_LM\"]:\n",
    "        LM_priors = {}\n",
    "        if priors_type == \"gaussian\":\n",
    "            for key in priors_dict.keys():\n",
    "                LM_priors[key] = [priors_dict[key][0]-3*priors_dict[key][1],\n",
    "                                  priors_dict[key][0]+3*priors_dict[key][1]]\n",
    "        else:\n",
    "            LM_priors = priors_dict\n",
    "        fit_params, fit_model, residuals, uncertainty = LMfit(times, wlc, exoplanet_params, systematics, limb_darkening_model,\n",
    "                                                              fixed_param, LM_priors, exoticLD, spectral_range)\n",
    "        # Update exoplanet_params based on fit_params.\n",
    "        systematics = (fit_params[\"a\"], fit_params[\"b\"])\n",
    "        for key in fit_params.keys():\n",
    "            if key not in (\"a\", \"b\"):\n",
    "                exoplanet_params[key] = fit_params[key]\n",
    "\n",
    "        if save_plots[\"WLC_LM\"]:\n",
    "            WLC_fitsdir = os.path.join(outdir, \"WLC_fits\")\n",
    "            if not os.path.exists(WLC_fitsdir):\n",
    "                os.makedirs(WLC_fitsdir)\n",
    "            # Plot the fit.\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.errorbar(times, wlc, yerr=np.full(shape=len(wlc), fill_value=uncertainty), fmt='o', color='black', label='obs', alpha=0.1, zorder=1)\n",
    "            plt.plot(times, fit_model, 'r-', label=\"fitted_function\", zorder=10)\n",
    "            plt.xlabel('time since mid-transit [days]')\n",
    "            plt.ylabel('relative flux [no units]')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            plt.savefig(os.path.join(WLC_fitsdir, \"wlc_fitLM.pdf\"), dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the residuals.\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.scatter(times, residuals)\n",
    "            plt.xlabel('time since mid-transit [days]')\n",
    "            plt.ylabel('normalized residuals')\n",
    "            plt.savefig(os.path.join(WLC_fitsdir, \"wlc_fitLM_residuals.pdf\"), dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    else:\n",
    "        uncertainty = 0.01 # high uncertainty if not pre-fit\n",
    "    \n",
    "    # Perform MCMC fit of wlc with initial guesses updated from fit_params.\n",
    "    original_exoplanet_params = deepcopy(exoplanet_params)\n",
    "    if do_fit[\"WLC_MCMC\"]:\n",
    "        # temporary fix on aoR, inc during MCMC\n",
    "        #fixed_param[\"aoR\"] = True\n",
    "        #fixed_param[\"impact\"] = True\n",
    "        #limb_darkening_model[\"initial_guess\"] = [0.1,0.1]\n",
    "        theta, depth, depth_err, model, residuals, res_err = MCMCfit(times, wlc, [uncertainty for i in wlc],\n",
    "                                                                     exoplanet_params, systematics,\n",
    "                                                                     limb_darkening_model,\n",
    "                                                                     fixed_param, exoticLD, spectral_range,\n",
    "                                                                     depth_type=MCMC_depth_type,\n",
    "                                                                     priors_dict=priors_dict,\n",
    "                                                                     priors_type=priors_type,\n",
    "                                                                     N_walkers = 80,\n",
    "                                                                     N_steps = 30000)\n",
    "        try:\n",
    "            print(\"Obtained grazing broadband depth of %.2f +/- %.2f.\" % (depth[2], depth_err[2]))\n",
    "        except:\n",
    "            print(\"Obtained broadband depth of %.2f +/- %.2f.\" % (depth, depth_err))\n",
    "\n",
    "        # Update exoplanet_params based on theta.\n",
    "        for key in theta.keys():\n",
    "            exoplanet_params[key] = theta[key]\n",
    "\n",
    "        if save_plots[\"WLC_MCMC\"]:\n",
    "            WLC_fitsdir = os.path.join(outdir, \"WLC_fits\")\n",
    "            if not os.path.exists(WLC_fitsdir):\n",
    "                os.makedirs(WLC_fitsdir)\n",
    "            fig, ax = plt.subplots(figsize=(20, 5))\n",
    "            ax.plot(times, wlc, lw=3)\n",
    "            ax.errorbar(times, wlc, yerr=[uncertainty for i in wlc], fmt=\"none\", capsize=3)\n",
    "            ax.plot(times, model, color=\"red\")\n",
    "            ax.set_xlabel(\"time since mid-transit [MJD]\")\n",
    "            ax.set_ylabel(\"normalized flux [DN/s]\")\n",
    "            ax.set_title(\"White light curve with MCMC fit\")\n",
    "            plt.savefig(os.path.join(WLC_fitsdir, \"wlc_fitMCMC.pdf\"), dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # Plot the residuals.\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.scatter(times, residuals)\n",
    "            plt.xlabel('time since mid-transit [days]')\n",
    "            plt.ylabel('normalized residuals')\n",
    "            plt.savefig(os.path.join(WLC_fitsdir, \"wlc_fitMCMC_residuals.pdf\"), dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    # Reset systematics.\n",
    "    systematics = original_systematics\n",
    "    \n",
    "    # Perform LM fits on spectroscopic light curves.\n",
    "    spectro_fixed_param = {\"LD_coeffs\":fixed_param[\"LD_coeffs\"],\n",
    "                           \"t0\":True,\n",
    "                           \"period\":True,\n",
    "                           \"aoR\":True,\n",
    "                           \"impact\":True,#\"inc\":True,\n",
    "                           \"ecc\":True,\n",
    "                           \"lop\":True,\n",
    "                           \"offset\":True}\n",
    "    slc_paths = sorted(glob.glob(os.path.join(curvesdir, \"slc*\")))\n",
    "    spec_uncertainties = []\n",
    "    spec_updated_guesses = []\n",
    "    \n",
    "    original_exoplanet_params = deepcopy(exoplanet_params)\n",
    "    LM_residuals = []\n",
    "    if do_fit[\"spec_LM\"]:\n",
    "        LM_priors = {}\n",
    "        if priors_type == \"gaussian\":\n",
    "            for key in priors_dict.keys():\n",
    "                LM_priors[key] = [priors_dict[key][0]-3*priors_dict[key][1],\n",
    "                                  priors_dict[key][0]+3*priors_dict[key][1]]\n",
    "        else:\n",
    "            LM_priors = priors_dict\n",
    "        for slc_path in slc_paths:\n",
    "            # These files have names slc_#.###mu_#.###mu.txt, can get spectral range out of these.\n",
    "            slc, times = read_light_curve(slc_path)\n",
    "            slc_file = str.split(slc_path, sep=\"/\")[-1]\n",
    "            savetag = slc_file[4:19]\n",
    "            min_wav = float(slc_file[4:9])\n",
    "            max_wav = float(slc_file[12:17])\n",
    "            slc_spectral_range = (min_wav, max_wav)\n",
    "\n",
    "            exoplanet_params = deepcopy(original_exoplanet_params) # prevents original guess from being modified.\n",
    "            print(exoplanet_params)\n",
    "            fit_params, fit_model, residuals, uncertainty = LMfit(times, slc, exoplanet_params, systematics, limb_darkening_model,\n",
    "                                                                  spectro_fixed_param, LM_priors, exoticLD, slc_spectral_range)\n",
    "            spec_updated_guesses.append(deepcopy(fit_params))\n",
    "            spec_uncertainties.append(uncertainty)\n",
    "            LM_residuals.append(residuals)\n",
    "\n",
    "            if save_plots[\"spec_LM\"]:\n",
    "                # Save plots of the LM fits to the spec curves.\n",
    "                spec_fitsdir = os.path.join(outdir, \"spec_fits\")\n",
    "                if not os.path.exists(spec_fitsdir):\n",
    "                    os.makedirs(spec_fitsdir)\n",
    "                # Plot the fit.\n",
    "                plt.figure(figsize=(20,5))\n",
    "                plt.errorbar(times, slc, yerr=np.full(shape=len(slc), fill_value=uncertainty), fmt='o', color='black', label='obs', alpha=0.1, zorder=1)\n",
    "                plt.plot(times, fit_model, 'r-', label=\"fitted_function\", zorder=10)\n",
    "                plt.xlabel('time since mid-transit [days]')\n",
    "                plt.ylabel('relative flux [no units]')\n",
    "                plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "                plt.savefig(os.path.join(spec_fitsdir, \"slc_fitLM_{}.pdf\".format(savetag)), dpi=300)\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                # Plot the residuals.\n",
    "                plt.figure(figsize=(20,5))\n",
    "                plt.scatter(times, residuals)\n",
    "                plt.xlabel('time since mid-transit [days]')\n",
    "                plt.ylabel('normalized residuals')\n",
    "                plt.savefig(os.path.join(spec_fitsdir, \"slc_fitLM_residuals_{}.pdf\".format(savetag)), dpi=300)\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "    else:\n",
    "        spec_uncertainties = [0.01 for i in slc_paths]\n",
    "        spec_updated_guesses = [original_exoplanet_params for i in slc_paths]\n",
    "    \n",
    "    # Now do MCMC fits.\n",
    "    rp_vals = []\n",
    "    rp_errs = []\n",
    "    alt_depths = []\n",
    "    alt_depth_errs = []\n",
    "    depths = []\n",
    "    depth_errs = []\n",
    "    wavelengths = []\n",
    "    halfwidths = []\n",
    "    original_exoplanet_params = deepcopy(exoplanet_params)\n",
    "    n_rejected = []\n",
    "    SDNRs = []\n",
    "    if do_fit[\"spec_MCMC\"]:\n",
    "        for slc_path, guess, uncertainty, residual in zip(slc_paths,spec_updated_guesses, spec_uncertainties, LM_residuals):\n",
    "            # These files have names slc_#.###mu_#.###mu.txt, can get spectral range out of these.\n",
    "            slc, times = read_light_curve(slc_path)\n",
    "            slc_file = str.split(slc_path, sep=\"/\")[-1]\n",
    "            savetag = slc_file[4:19]\n",
    "            min_wav = float(slc_file[4:9])\n",
    "            max_wav = float(slc_file[12:17])\n",
    "            slc_spectral_range = (min_wav, max_wav)\n",
    "            wavelengths.append((min_wav+max_wav)/2)\n",
    "            halfwidths.append((max_wav-min_wav)/2)\n",
    "            \n",
    "            # Use LM residuals to spot outliers from the fit and delete them.\n",
    "            slc, times, n_reject = reject_outliers(slc, times, residual, sigma=reject_threshold, raise_alarm=raise_alarm)\n",
    "\n",
    "            # guess has to be used to update exoplanet_params.\n",
    "            exoplanet_params = deepcopy(original_exoplanet_params) # prevents original guess from being modified.\n",
    "            for key in exoplanet_params.keys():\n",
    "                if key not in guess.keys():\n",
    "                    guess[key] = exoplanet_params[key]\n",
    "\n",
    "            theta, depth, depth_err, model, residuals, res_err = MCMCfit(times, slc, [uncertainty for i in slc],\n",
    "                                                                         guess, systematics,\n",
    "                                                                         limb_darkening_model,\n",
    "                                                                         spectro_fixed_param, exoticLD, slc_spectral_range,\n",
    "                                                                         depth_type=MCMC_depth_type,\n",
    "                                                                         priors_dict=priors_dict,\n",
    "                                                                         priors_type=priors_type)\n",
    "            print(\"For wavelength range {}, {}:\".format(min_wav, max_wav))\n",
    "            try:\n",
    "                # Outputs as 100*rp/rs, standard, ldcta.\n",
    "                rp_vals.append(depth[0])\n",
    "                rp_errs.append(depth_err[0])\n",
    "                \n",
    "                depths.append(depth[2])\n",
    "                depth_errs.append(depth_err[2])\n",
    "                \n",
    "                alt_depths.append(depth[1])\n",
    "                alt_depth_errs.append(depth_err[1])\n",
    "                \n",
    "                print(\"Obtained overlap depth %.2f +/- %.2f, at SDNR = %.2f ppm.\" % (depth[2], depth_err[2], np.std(residuals)*10**6))\n",
    "            except:\n",
    "                print(\"Obtained depth %.2f +/- %.2f, at SDNR = %.2f ppm.\" % (depth, depth_err, np.std(residuals)*10**6))\n",
    "                depths.append(depth)\n",
    "                depth_errs.append(depth_err)\n",
    "            \n",
    "            n_rejected.append(n_reject)\n",
    "            SDNRs.append(np.std(residuals)*10**6)\n",
    "            if save_plots[\"spec_MCMC\"]:\n",
    "                # Save plots of fits.\n",
    "                spec_fitsdir = os.path.join(outdir, \"spec_fits\")\n",
    "                if not os.path.exists(spec_fitsdir):\n",
    "                    os.makedirs(spec_fitsdir)\n",
    "                fig, ax = plt.subplots(figsize=(20, 5))\n",
    "                ax.plot(times, slc, lw=3)\n",
    "                ax.errorbar(times, slc, yerr=[uncertainty for i in slc], fmt=\"none\", capsize=3)\n",
    "                ax.plot(times, model, color=\"red\")\n",
    "                ax.set_xlabel(\"time since mid-transit [MJD]\")\n",
    "                ax.set_ylabel(\"normalized flux [DN/s]\")\n",
    "                ax.set_title(\"Spectroscopic light curve (%.3f micron) with MCMC fit\" % wavelengths[-1])\n",
    "                plt.savefig(os.path.join(spec_fitsdir, \"slc_fitMCMC_{}.pdf\".format(savetag)), dpi=300)\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                # Plot the residuals.\n",
    "                plt.figure(figsize=(20,5))\n",
    "                plt.scatter(times, residuals)\n",
    "                plt.xlabel('time since mid-transit [days]')\n",
    "                plt.ylabel('normalized residuals')\n",
    "                plt.savefig(os.path.join(spec_fitsdir, \"slc_fitMCMC_residuals_{}.pdf\".format(savetag)), dpi=300)\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        if save_plots[\"spec_MCMC\"]:\n",
    "            # Save plots of spec curve MCMC results.\n",
    "            try:\n",
    "                fig, ax = plot_transit_spectrum(wavelengths, depths, depth_errs,\n",
    "                                                ymin=0.95*min(depths), ymax=1.05*max(depths))\n",
    "                spec_outdir = os.path.join(outdir, \"spectrum\")\n",
    "                if not os.path.exists(spec_outdir):\n",
    "                    os.makedirs(spec_outdir)\n",
    "                plt.savefig(os.path.join(spec_outdir, \"slc_transitspectrum.pdf\"), dpi=300)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "            except:\n",
    "                print(\"Error encountered in plotting depths, passing...\")\n",
    "            \n",
    "            try:\n",
    "                if rp_vals:\n",
    "                    fig, ax = plot_transit_spectrum(wavelengths, rp_vals, rp_errs,\n",
    "                                                ymin=0.95*min(rp_vals), ymax=1.05*max(rp_vals))\n",
    "                    spec_outdir = os.path.join(outdir, \"spectrum\")\n",
    "                    if not os.path.exists(spec_outdir):\n",
    "                        os.makedirs(spec_outdir)\n",
    "                    plt.savefig(os.path.join(spec_outdir, \"slc_transitspectrum_rprs.pdf\"), dpi=300)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "            except:\n",
    "                print(\"Error encountered in plotting depths, passing...\")\n",
    "            \n",
    "            try:\n",
    "                if alt_depths:\n",
    "                    fig, ax = plot_transit_spectrum(wavelengths, alt_depths, alt_depth_errs,\n",
    "                                                ymin=0.95*min(alt_depths), ymax=1.05*max(alt_depths))\n",
    "                    spec_outdir = os.path.join(outdir, \"spectrum\")\n",
    "                    if not os.path.exists(spec_outdir):\n",
    "                        os.makedirs(spec_outdir)\n",
    "                    plt.savefig(os.path.join(spec_outdir, \"slc_transitspectrum_standard.pdf\"), dpi=300)\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "            except:\n",
    "                print(\"Error encountered in plotting depths, passing...\")\n",
    "\n",
    "        # Write out transit spectrum.\n",
    "        spec_outdir = os.path.join(outdir, \"spectrum\")\n",
    "        try:\n",
    "            write_transit_spectrum(wavelengths, halfwidths, depths, depth_errs, spec_outdir)\n",
    "        except:\n",
    "            print(\"Error encountered in saving depths, passing...\")\n",
    "        \n",
    "        try:\n",
    "            if rp_vals:\n",
    "                spec_outdir = os.path.join(outdir, \"spectrum_rprs\")\n",
    "                write_transit_spectrum(wavelengths, halfwidths, rp_vals, rp_errs, spec_outdir)\n",
    "        except:\n",
    "            print(\"Error encountered in saving depths, passing...\")\n",
    "            \n",
    "        try:\n",
    "            if alt_depths:\n",
    "                spec_outdir = os.path.join(outdir, \"spectrum_rprs2\")\n",
    "                write_transit_spectrum(wavelengths, halfwidths, alt_depths, alt_depth_errs, spec_outdir)\n",
    "        with open(os.path.join(outdir,\"{}_sigma_SDNR.txt\".format(reject_threshold))) as f:\n",
    "            f.write(\"wavelength [mu]    n_reject [na]    SDNR [ppm]:\\n\")\n",
    "            for wav, n_reject, SDNR in zip(wavelengths, n_rejected, SDNRs):\n",
    "                f.write(\"{}    {}    {}\\n\".format(wav, n_reject, SDNR))\n",
    "\n",
    "        except:\n",
    "            print(\"Error encountered in saving depths, passing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1773d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_light_curve(filepath):\n",
    "    '''\n",
    "    Reads out the light curve .txt located at filepath.\n",
    "    \n",
    "    :param filepath: str. Where the light curve .txt object is located.\n",
    "    :return: lc_n, t object.\n",
    "    '''\n",
    "    lc = []\n",
    "    t = []\n",
    "    with open(filepath) as f:\n",
    "            line = f.readline() \n",
    "            while line[0] == '#':\n",
    "                # Read past comments.\n",
    "                line = f.readline()\n",
    "            while line != '':\n",
    "                line = str.split(line)#, sep='   ')\n",
    "                \n",
    "                # Extract useful info.\n",
    "                time = float(line[0]) # time in days relative to mid-transit\n",
    "                flux = float(str.replace(line[1],'\\n','')) # normalized flux\n",
    "                \n",
    "                t.append(time)\n",
    "                lc.append(flux)\n",
    "                \n",
    "                line = f.readline()\n",
    "    return np.array(lc), np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c09a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMfit(t, lc, exoplanet_params, systematics, limb_darkening_model, fixed_param, priors_dict, exoticLD, spectral_range):\n",
    "    '''\n",
    "    Performs Levenberg-Marquardt fit of transit model to provided light curve.\n",
    "    '''\n",
    "    # Uses Levenberg-Marquardt to fit a transit model to the transit data defined by t, lc_n using\n",
    "    # a, b, exoplanet_params[0]=t0, exoplanet_params[1]=rp as the initial guess and using\n",
    "    # exoplanet_params = [t0, period, rp, smax, inclination, eccen, lop]\n",
    "    # to describe the system. Needs wavmin, wavmax to get the ld coefficients.\n",
    "    # Returns the mid-transit time fitted from this and also returns the err = np.std(residuals).\n",
    "    # Saves a plot of the lc_n fit as well.\n",
    "    # Input the stellar parameters M_H (metallicity in dex), Teff (temperature in K),\n",
    "    # and log g (log10 of the dimensionless surface gravity) to get ld coeffs.\n",
    "    # Also input the ld_data_path, the path to where the exotic-ld data is stored.\n",
    "    \n",
    "    # Begin unpacking systematics and stellar_params.\n",
    "    a, b = systematics\n",
    "    exoplanet_params[\"model_type\"], stellar_params, exoplanet_params[\"LD_coeffs\"] = (limb_darkening_model[\"model_type\"],\n",
    "                                                                                     limb_darkening_model[\"stellar_params\"],\n",
    "                                                                                     limb_darkening_model[\"initial_guess\"])\n",
    "    \n",
    "    if (exoticLD[\"available\"]):\n",
    "        # Check for custom model.\n",
    "        if exoticLD[\"ld_grid\"] == \"custom\":\n",
    "            file_path = exoticLD[\"custom_model_path\"]\n",
    "\n",
    "            s_wvs = (np.genfromtxt(file_path, skip_header = 2, usecols = [0]).T)*1e4\n",
    "            s_mus = np.flip(np.genfromtxt(file_path, skip_header = 1, max_rows = 1))\n",
    "            stellar_intensity = np.flip(np.genfromtxt(file_path, skip_header = 2)[:,1:],axis = 1)\n",
    "\n",
    "            sld = SLD(ld_data_path=exoticLD[\"ld_data_path\"], ld_model=\"custom\",\n",
    "                      custom_wavelengths=s_wvs, custom_mus=s_mus, custom_stellar_model=stellar_intensity)\n",
    "\n",
    "            exoplanet_params[\"LD_coeffs\"] = sld.compute_quadratic_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                            mode=\"JWST_NIRSpec_prism\")\n",
    "            exoplanet_params[\"LD_coeffs\"] = [exoplanet_params[\"LD_coeffs\"][0], exoplanet_params[\"LD_coeffs\"][1]+exoplanet_params[\"offset\"]]\n",
    "\n",
    "        else:\n",
    "        # Get LD coefficients from EXoTiC-LD using standard grids.\n",
    "            M_H, Teff, logg = stellar_params\n",
    "\n",
    "            sld = SLD(M_H=M_H, Teff=Teff, logg=logg,\n",
    "                      ld_model=exoticLD[\"ld_grid\"], ld_data_path=exoticLD[\"ld_data_path\"],\n",
    "                      interpolate_type=exoticLD[\"ld_interpolate_type\"], verbose=True)\n",
    "\n",
    "            if exoplanet_params[\"model_type\"] in (\"quadratic\", \"kipping2013\"):\n",
    "                exoplanet_params[\"LD_coeffs\"] = sld.compute_quadratic_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "            if exoplanet_params[\"model_type\"] == \"square-root\":\n",
    "                exoplanet_params[\"LD_coeffs\"] = sld.compute_squareroot_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                 mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "            if exoplanet_params[\"model_type\"] == \"nonlinear\":\n",
    "                exoplanet_params[\"LD_coeffs\"] = sld.compute_4_parameter_non_linear_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                             mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "        \n",
    "    # Special condition for those using Kipping2013.\n",
    "    if (exoplanet_params[\"model_type\"] == \"kipping2013\"):# and not exoticLD[\"available\"]):\n",
    "        print(\"Kipping2013 formulation found, switching coefficients...\")\n",
    "        using_kipping = True\n",
    "        exoplanet_params[\"model_type\"] = \"quadratic\"\n",
    "        q1, q2 = limb_darkening_model[\"initial_guess\"]\n",
    "        u1 = 2*np.sqrt(q1)*q2\n",
    "        u2 = np.sqrt(q1)*(1-2*q2)\n",
    "        exoplanet_params[\"LD_coeffs\"] = [u1, u2] # modify guess to Kipping parameters\n",
    "    else:\n",
    "        using_kipping = False\n",
    "    \n",
    "    # Set up params for batman\n",
    "    params = batman.TransitParams()\n",
    "    params.t0 = exoplanet_params[\"t0\"]                       #time of inferior conjunction in days\n",
    "    params.per = exoplanet_params[\"period\"]                  #orbital period in days\n",
    "    params.rp = exoplanet_params[\"rp\"]                       #planet radius (in units of stellar radii)\n",
    "    params.a = exoplanet_params[\"aoR\"]                       #semi-major axis (in units of stellar radii)\n",
    "    params.inc = (180/np.pi)*np.arccos(exoplanet_params[\"impact\"]/params.a)#exoplanet_params[\"inc\"]                     #orbital inclination (in degrees)\n",
    "    params.ecc = exoplanet_params[\"ecc\"]                     #eccentricity\n",
    "    params.w = exoplanet_params[\"lop\"]                       #longitude of periastron (in degrees)\n",
    "    params.u = exoplanet_params[\"LD_coeffs\"]                 #limb darkening coefficients [u1, u2]\n",
    "    params.limb_dark = exoplanet_params[\"model_type\"]        #limb darkening model\n",
    "    \n",
    "    print(\"Using ld coeffs initial guess: \", params.u)\n",
    "    residuals, theta_guess, fit_theta, fit_corr_bat_lc = jwst_ls(exoplanet_params, fixed_param, priors_dict,\n",
    "                                                                 a, b, t, lc, using_kipping)\n",
    "    \n",
    "    err = np.std(residuals)\n",
    "    print(\"Standard deviation of the residuals: %.0f ppm\" % (err*10**6))\n",
    "    \n",
    "    return fit_theta, fit_corr_bat_lc, residuals, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jwst_ls(exoplanet_params, fixed_param, priors_dict, a, b, t, lc, using_kipping):\n",
    "    #---BATMAN MODEL-----\n",
    "    params = batman.TransitParams()\n",
    "    params.t0 = exoplanet_params[\"t0\"]                       #time of inferior conjunction in days\n",
    "    params.per = exoplanet_params[\"period\"]                  #orbital period in days\n",
    "    params.rp = exoplanet_params[\"rp\"]                       #planet radius (in units of stellar radii)\n",
    "    params.a = exoplanet_params[\"aoR\"]                       #semi-major axis (in units of stellar radii)\n",
    "    params.inc = (180/np.pi)*np.arccos(exoplanet_params[\"impact\"]/params.a)#exoplanet_params[\"inc\"]                     #orbital inclination (in degrees)\n",
    "    params.ecc = exoplanet_params[\"ecc\"]                     #eccentricity\n",
    "    params.w = exoplanet_params[\"lop\"]                       #longitude of periastron (in degrees)\n",
    "    params.u = exoplanet_params[\"LD_coeffs\"]                 #limb darkening coefficients [u1, u2]\n",
    "    params.limb_dark = exoplanet_params[\"model_type\"]        #limb darkening model\n",
    "    \n",
    "    m = batman.TransitModel(params, t)                       #initializes model\n",
    "    f = m.light_curve(params)                                #calculates light curve\n",
    "    init_bat_model = batman.TransitModel(params, t)\n",
    "    \n",
    "    #---DEFINE GUESSES-----\n",
    "    theta_guess = {}\n",
    "    \n",
    "    theta_guess[\"a\"] = a\n",
    "    theta_guess[\"b\"] = b\n",
    "    theta_guess[\"rp\"] = params.rp\n",
    "    \n",
    "    for parameter in fixed_param.keys():\n",
    "        if not fixed_param[parameter]:\n",
    "            theta_guess[parameter] = exoplanet_params[parameter]\n",
    "    \n",
    "    #---LEAST SQUARES FITTING-----\n",
    "    flux_err = np.full(shape=len(lc), fill_value=np.std(lc))\n",
    "    \n",
    "    fit_theta, fit_theta_arr, num_of_ld_coeffs, modified_keys, fit_corr_bat_lc, fit_bat_lc, polyfit, original_LDs = fit_model(theta_guess, init_bat_model,\n",
    "                                                                                                                              params, t, lc, flux_err, priors_dict, using_kipping)\n",
    "    \n",
    "    residuals = residuals_(fit_theta_arr, fit_theta, num_of_ld_coeffs,\n",
    "                           modified_keys, init_bat_model, params, t, lc, original_LDs, using_kipping)\n",
    "    \n",
    "    return residuals, theta_guess, fit_theta, fit_corr_bat_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca513e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(theta_guess, init_bat_model, params, t, flux, flux_err, priors_dict, using_kipping):\n",
    "    original_theta_guess = deepcopy(theta_guess)\n",
    "    # Need to unpack the limb darkening coefficients and build bounds object\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    \n",
    "    # Check limb dark type\n",
    "    if (params.limb_dark == \"quadratic\" and using_kipping):\n",
    "        print(\"Kipping2013 formulation found, switching priors appropriately...\")\n",
    "        ld_lower = 0\n",
    "        ld_upper = 1\n",
    "    \n",
    "    else:\n",
    "        ld_lower = -1\n",
    "        ld_upper = 2\n",
    "        \n",
    "    theta_arr = []\n",
    "    num_of_ld_coeffs = 0\n",
    "    modified_keys = []\n",
    "    for key in theta_guess.keys():\n",
    "        if key==\"LD_coeffs\":\n",
    "            for ld_coeff in theta_guess[key]:\n",
    "                num_of_ld_coeffs += 1\n",
    "                theta_arr.append(ld_coeff)\n",
    "                modified_keys.append(\"LD_coeff\")\n",
    "                lower_bounds.append(ld_lower)\n",
    "                upper_bounds.append(ld_upper)\n",
    "        elif key in priors_dict.keys():\n",
    "            theta_arr.append(theta_guess[key])\n",
    "            modified_keys.append(key)\n",
    "            lower_bounds.append(priors_dict[key][0])\n",
    "            upper_bounds.append(priors_dict[key][1])\n",
    "        else:\n",
    "            theta_arr.append(theta_guess[key])\n",
    "            modified_keys.append(key)\n",
    "            lower_bounds.append(-np.inf)\n",
    "            upper_bounds.append(np.inf)\n",
    "    bounds=(lower_bounds, upper_bounds)\n",
    "    original_LDs = deepcopy(params.u)\n",
    "    opt_result = least_squares(residuals_,\n",
    "                               np.array(theta_arr),\n",
    "                               bounds=bounds,\n",
    "                               args=(theta_guess, num_of_ld_coeffs, modified_keys, init_bat_model, params, t, flux, original_LDs, using_kipping))\n",
    "    fit_theta_arr = opt_result.x\n",
    "    print(\"Fitted: \", fit_theta_arr)\n",
    "    print(\"Least squares finished with status:\", opt_result.status)\n",
    "    print(\"Output message: \", opt_result.message)\n",
    "    print(\"Success status: \", opt_result.success)\n",
    "    # Return fit_theta to dictionary format, and unpack fitted LD coeffs back into list.\n",
    "    fit_theta_dict = {}\n",
    "    if \"LD_coeff\" in modified_keys:\n",
    "        fitted_LD_coeffs = []\n",
    "    for fitted_param, modified_key in zip(fit_theta_arr, modified_keys):\n",
    "        if \"LD_coeff\" != modified_key:\n",
    "            fit_theta_dict[modified_key] = fitted_param\n",
    "        else:\n",
    "            fitted_LD_coeffs.append(fitted_param)\n",
    "    if \"LD_coeff\" in modified_keys:\n",
    "        fit_theta_dict[\"LD_coeffs\"] = fitted_LD_coeffs\n",
    "    fit_theta = fit_theta_dict\n",
    "    \n",
    "    rchi2 = (residuals_(fit_theta_arr, fit_theta, num_of_ld_coeffs, modified_keys, init_bat_model, params, t, flux, original_LDs, using_kipping)**2).sum()/(len(flux)-len(theta_guess))\n",
    "    \n",
    "    print('Guess', original_theta_guess)\n",
    "    print('Fitted', fit_theta)\n",
    "    \n",
    "    fit_corr_bat_lc, fit_bat_lc, linearfit = modify_model(fit_theta, init_bat_model, params, t, original_LDs)\n",
    "    res = residuals_(fit_theta_arr, fit_theta, num_of_ld_coeffs, modified_keys, init_bat_model, params, t, flux, original_LDs, using_kipping)\n",
    "    \n",
    "    chi2 = sum(res*res)\n",
    "    print('Chi-square =', chi2)\n",
    "    \n",
    "    dof = len(flux)-len(theta_guess)\n",
    "    print('Deg of freedom =', dof)\n",
    "    \n",
    "    print('Reduced Chi-square =', rchi2)\n",
    "    return fit_theta, fit_theta_arr, num_of_ld_coeffs, modified_keys, fit_corr_bat_lc, fit_bat_lc, linearfit, original_LDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_(theta_arr, theta, num_of_ld_coeffs, modified_keys, init_bat_model, params, t, flux, original_LDs, using_kipping):\n",
    "    # theta-arr is the array which will be modified. theta is the dictionary to which these\n",
    "    # these changes must be broadcast back to. Need to be delicate handling LD coeffs.\n",
    "    if \"LD_coeff\" in modified_keys:\n",
    "        LD_coeffs = []\n",
    "    checked = 0\n",
    "    i = 0\n",
    "    for theta_arr_item, modified_key in zip(theta_arr, modified_keys):\n",
    "        if \"LD_coeff\" not in modified_key:\n",
    "            theta[modified_key] = theta_arr_item\n",
    "        else:\n",
    "            if using_kipping:\n",
    "                if checked == 0:\n",
    "                    u1 = 2*np.sqrt(theta_arr[i])*theta_arr[i+1]\n",
    "                    LD_coeffs.append(u1)\n",
    "                    checked += 1\n",
    "                else:\n",
    "                    u2 = np.sqrt(theta_arr[i-1])*(1-2*theta_arr[i])\n",
    "                    LD_coeffs.append(u2)\n",
    "            else:\n",
    "                LD_coeffs.append(theta_arr_item)\n",
    "        i += 1\n",
    "    if \"LD_coeff\" in modified_keys:\n",
    "        theta[\"LD_coeffs\"] = LD_coeffs\n",
    "    \n",
    "    full_bat_model, model_lc, linearfit = modify_model(theta, init_bat_model, params, t, original_LDs)\n",
    "    \n",
    "    residuals = (flux-full_bat_model)\n",
    "    \n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model(theta, init_bat_model, params, t, original_LDs):\n",
    "    a = theta[\"a\"]\n",
    "    b = theta[\"b\"]\n",
    "    params.rp = theta[\"rp\"]\n",
    "    \n",
    "    fit_params = theta.keys()\n",
    "    if \"LD_coeffs\" in fit_params:\n",
    "        params.u = theta[\"LD_coeffs\"]\n",
    "    if \"offset\" in fit_params:\n",
    "        params.u = [original_LDs[0], original_LDs[1] + theta[\"offset\"]]\n",
    "    if \"t0\" in fit_params:\n",
    "        params.t0 = theta[\"t0\"]\n",
    "    if \"period\" in fit_params:\n",
    "        params.per = theta[\"period\"]\n",
    "    if \"aoR\" in fit_params:\n",
    "        params.a = theta[\"aoR\"]\n",
    "    #if \"inc\" in fit_params:\n",
    "    if \"impact\" in fit_params:\n",
    "        params.inc = (180/np.pi)*np.arccos(theta[\"impact\"]/params.a) #theta[\"inc\"]\n",
    "    if \"ecc\" in fit_params:\n",
    "        params.ecc = theta[\"ecc\"]\n",
    "    if \"lop\" in fit_params:\n",
    "        params.w = theta[\"lop\"]\n",
    "    \n",
    "    model_lc = init_bat_model.light_curve(params)\n",
    "    polyfit = a + b*t\n",
    "    \n",
    "    full_bat_model = model_lc*polyfit\n",
    "    \n",
    "    return full_bat_model, model_lc, polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30547117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMCfit(t, lc, err, exoplanet_params, systematics,\n",
    "            limb_darkening_model, fixed_param, exoticLD, spectral_range, depth_type=\"standard\",\n",
    "            priors_dict={\"t0\":[-0.1, 0.1],\n",
    "                         \"period\":[0, 100],\n",
    "                         \"aoR\":[0.00001, 10],\n",
    "                         \"impact\":[0, 100],#\"inc\":[80,90],\n",
    "                         \"ecc\":[0, 1],\n",
    "                         \"lop\":[0, 90],\n",
    "                         \"offset\":[-0.5, 0.5]},\n",
    "            priors_type=\"uniform\",\n",
    "            N_walkers = 32,\n",
    "            N_steps = 5000):\n",
    "    '''\n",
    "    Fits a transit model to the given transit curve using MCMC.\n",
    "    \n",
    "    :param t: 1D array. Timestamps of each point in the transit curve, in days.\n",
    "    :param lc: 1D array. The normalized flux at each point in the transit curve.\n",
    "    :param err: 1D array. Errors on the transit curve.\n",
    "    :param exoplanet_params: dict of float. Contains keywords \"t0\", \"period\", \"rp\", \"aoR\", \"inc\", \"ecc\", \"lop\".\n",
    "    :param central_wavelength: float. The central wavelength of the transit.\n",
    "    :param halfwidth: float. The halfwidth of the wavelength bin.\n",
    "    :param systematics: tuple of float. Contains parameters for a linear-in-time fit a+b*(x-0.5).\n",
    "    :param limb_darkening_model: dict. Contains \"model_type\" str which defines model choice (e.g. quadratic, 4-param),\n",
    "                                 \"stellar_params\" tuple of (M_H, Teff, logg) or None if not using, \"initial_guess\"\n",
    "                                 keyword containing tuple of floats. The permissable model types are \"quadratic\",\n",
    "                                 \"kipping2013\", \"square-root\", \"nonlinear\".\n",
    "    :param fixed_param. dict of bools. Keywords are parameters that can be held fixed or opened for fitting.\n",
    "                        If True, parameter will be held fixed. If False, parameter is allowed to be fitted.\n",
    "    :param exoticLD: dict. Contains \"available\" bool for whether EXoTiC-LD is on this system,\n",
    "                     \"ld_data_path\" str of where the exotic_ld_data directory is located.\n",
    "    :param spectral_range: tuple of float. Spectral range being covered.\n",
    "    :param depth_type: str. Choices are \"standard\", \"ldcta\" \"rp/rs\", or \"all\" for (rp/rs)^2,\n",
    "                       limb darkening-corrected time averaged, rp/rs, or to output all types.\n",
    "                       \"ldcta\" is for grazing tranist geometries.\n",
    "    :param priors_dict: dict. Priors for each parameter. If type \"uniform\", each\n",
    "                        entry is a list of the endpoints. If type \"gaussian\", each \n",
    "                        entry is mu and sigma.\n",
    "    :param priors_type: str. Choices are \"uniform\" or \"gaussian\". Type of priors to use.\n",
    "    :return: theta, depth, depth_err, model, residuals, and err = sdnr of residuals.\n",
    "    '''\n",
    "    try:\n",
    "        # Begin unpacking systematics and stellar_params.\n",
    "        a, b = systematics\n",
    "        exoplanet_params[\"model_type\"], stellar_params, exoplanet_params[\"LD_coeffs\"] = (limb_darkening_model[\"model_type\"],\n",
    "                                                                                         limb_darkening_model[\"stellar_params\"],\n",
    "                                                                                         limb_darkening_model[\"initial_guess\"])\n",
    "        \n",
    "        if (exoticLD[\"available\"]):\n",
    "            # Check for custom model.\n",
    "            if exoticLD[\"ld_grid\"] == \"custom\":\n",
    "                file_path = exoticLD[\"custom_model_path\"]\n",
    "                \n",
    "                s_wvs = (np.genfromtxt(file_path, skip_header = 2, usecols = [0]).T)*1e4\n",
    "                s_mus = np.flip(np.genfromtxt(file_path, skip_header = 1, max_rows = 1))\n",
    "                stellar_intensity = np.flip(np.genfromtxt(file_path, skip_header = 2)[:,1:],axis = 1)\n",
    "                \n",
    "                sld = SLD(ld_data_path=exoticLD[\"ld_data_path\"], ld_model=\"custom\",\n",
    "                          custom_wavelengths=s_wvs, custom_mus=s_mus, custom_stellar_model=stellar_intensity)\n",
    "                \n",
    "                exoplanet_params[\"LD_coeffs\"] = sld.compute_quadratic_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                mode=\"JWST_NIRSpec_prism\")\n",
    "                exoplanet_params[\"LD_coeffs\"] = [exoplanet_params[\"LD_coeffs\"][0], exoplanet_params[\"LD_coeffs\"][1]+exoplanet_params[\"offset\"]]\n",
    "\n",
    "            else:\n",
    "            # Get LD coefficients from EXoTiC-LD using standard grids.\n",
    "                M_H, Teff, logg = stellar_params\n",
    "\n",
    "                sld = SLD(M_H=M_H, Teff=Teff, logg=logg,\n",
    "                          ld_model=exoticLD[\"ld_grid\"], ld_data_path=exoticLD[\"ld_data_path\"],\n",
    "                          interpolate_type=exoticLD[\"ld_interpolate_type\"], verbose=True)\n",
    "\n",
    "                if exoplanet_params[\"model_type\"] in (\"quadratic\", \"kipping2013\"):\n",
    "                    exoplanet_params[\"LD_coeffs\"] = sld.compute_quadratic_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                    mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "                if exoplanet_params[\"model_type\"] == \"square-root\":\n",
    "                    exoplanet_params[\"LD_coeffs\"] = sld.compute_squareroot_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                     mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "                if exoplanet_params[\"model_type\"] == \"nonlinear\":\n",
    "                    exoplanet_params[\"LD_coeffs\"] = sld.compute_4_parameter_non_linear_ld_coeffs(wavelength_range=[(10**4)*spectral_range[0], (10**4)*spectral_range[1]],\n",
    "                                                                                                 mode=\"JWST_NIRSpec_prism\", mu_min=0.0)\n",
    "        \n",
    "        # Special condition for those using Kipping2013.\n",
    "        if (exoplanet_params[\"model_type\"] == \"kipping2013\"):# and not exoticLD[\"available\"]):\n",
    "            print(\"Kipping2013 formulation found, switching coefficients...\")\n",
    "            using_kipping = True\n",
    "            exoplanet_params[\"model_type\"] = \"quadratic\"\n",
    "            q1, q2 = limb_darkening_model[\"initial_guess\"]\n",
    "            u1 = 2*np.sqrt(q1)*q2\n",
    "            u2 = np.sqrt(q1)*(1-2*q2)\n",
    "            exoplanet_params[\"LD_coeffs\"] = [u1, u2] # modify guess to Kipping parameters\n",
    "        else:\n",
    "            using_kipping = False\n",
    "        \n",
    "        # Initialize the batman transit model.\n",
    "        \n",
    "        params = batman.TransitParams()\n",
    "        params.t0 = exoplanet_params[\"t0\"]                       #time of inferior conjunction in days\n",
    "        params.per = exoplanet_params[\"period\"]                  #orbital period in days\n",
    "        params.rp = exoplanet_params[\"rp\"]                       #planet radius (in units of stellar radii)\n",
    "        params.a = exoplanet_params[\"aoR\"]                       #semi-major axis (in units of stellar radii)\n",
    "        params.inc = (180/np.pi)*np.arccos(exoplanet_params[\"impact\"]/params.a)#exoplanet_params[\"inc\"]                     #orbital inclination (in degrees)\n",
    "        params.ecc = exoplanet_params[\"ecc\"]                     #eccentricity\n",
    "        params.w = exoplanet_params[\"lop\"]                       #longitude of periastron (in degrees)\n",
    "        params.u = exoplanet_params[\"LD_coeffs\"]                 #limb darkening coefficients [u1, u2]\n",
    "        params.limb_dark = exoplanet_params[\"model_type\"]        #limb darkening model\n",
    "        \n",
    "        print(\"Using ld coeffs initial guess: \", params.u)\n",
    "        \n",
    "        m = batman.TransitModel(params, t)                       #initializes model\n",
    "        f = m.light_curve(params)                                #calculates light curve\n",
    "        init_bat_model = batman.TransitModel(params, t)\n",
    "        \n",
    "        # Initialize the guess.\n",
    "        theta_guess = {}\n",
    "    \n",
    "        theta_guess[\"a\"] = a\n",
    "        theta_guess[\"b\"] = b\n",
    "        theta_guess[\"rp\"] = params.rp\n",
    "    \n",
    "        for parameter in fixed_param.keys():\n",
    "            if not fixed_param[parameter]:\n",
    "                theta_guess[parameter] = exoplanet_params[parameter]\n",
    "        original_theta_guess = deepcopy(theta_guess)\n",
    "        \n",
    "        # Convert the guess into an array.\n",
    "        theta_arr = []\n",
    "        num_of_ld_coeffs = 0\n",
    "        modified_keys = []\n",
    "        for key in theta_guess.keys():\n",
    "            if key==\"LD_coeffs\":\n",
    "                for ld_coeff in theta_guess[key]:\n",
    "                    num_of_ld_coeffs += 1\n",
    "                    theta_arr.append(ld_coeff)\n",
    "                    modified_keys.append(\"LD_coeff\")\n",
    "            else:\n",
    "                theta_arr.append(theta_guess[key])\n",
    "                modified_keys.append(key)\n",
    "        theta_arr = np.array(theta_arr)\n",
    "        \n",
    "        # We redefine these functions in terms of the batman model.\n",
    "        # Defines the log-likelihood.\n",
    "        def log_likelihood(theta, modified_keys, num_of_ld_coeffs, x, y, yerr, original_LDs):\n",
    "            # theta is an array. Need to turn it back into a dictionary.\n",
    "            theta_dict = {}\n",
    "            if \"LD_coeff\" in modified_keys:\n",
    "                LD_coeffs = []\n",
    "            for theta_arr_item, modified_key in zip(theta, modified_keys):\n",
    "                if \"LD_coeff\" not in modified_key:\n",
    "                    theta_dict[modified_key] = theta_arr_item\n",
    "                else:\n",
    "                    LD_coeffs.append(theta_arr_item)\n",
    "            if \"LD_coeff\" in modified_keys:\n",
    "                theta_dict[\"LD_coeffs\"] = LD_coeffs\n",
    "            \n",
    "            a = theta_dict[\"a\"]\n",
    "            b = theta_dict[\"b\"]\n",
    "            params.rp = theta_dict[\"rp\"]\n",
    "            \n",
    "            fit_params = theta_dict.keys()\n",
    "            if \"LD_coeffs\" in fit_params:\n",
    "                params.u = theta_dict[\"LD_coeffs\"]\n",
    "            if \"offset\" in fit_params:\n",
    "                params.u = [original_LDs[0], original_LDs[1] + theta_dict[\"offset\"]]\n",
    "            if \"t0\" in fit_params:\n",
    "                params.t0 = theta_dict[\"t0\"]\n",
    "            if \"period\" in fit_params:\n",
    "                params.per = theta_dict[\"period\"]\n",
    "            if \"aoR\" in fit_params:\n",
    "                params.a = theta_dict[\"aoR\"]\n",
    "            #if \"inc\" in fit_params:\n",
    "            if \"impact\" in fit_params:\n",
    "                params.inc = (180/np.pi)*np.arccos(theta_dict[\"impact\"]/params.a)#theta_dict[\"inc\"]\n",
    "            if \"ecc\" in fit_params:\n",
    "                params.ecc = theta_dict[\"ecc\"]\n",
    "            if \"lop\" in fit_params:\n",
    "                params.w = theta_dict[\"lop\"]\n",
    "            \n",
    "            if using_kipping:\n",
    "                # Change to Kipping parameters\n",
    "                q1, q2 = LD_coeffs\n",
    "                u1 = 2*np.sqrt(q1)*q2\n",
    "                u2 = np.sqrt(q1)*(1-2*q2)\n",
    "                params.u = [u1, u2]\n",
    "            \n",
    "            polyfit= a + b*x\n",
    "            \n",
    "            model = m.light_curve(params)*polyfit\n",
    "            sigma2 = yerr**2\n",
    "            return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "        # Defines the prior.\n",
    "        def log_prior(theta, modified_keys, num_of_ld_coeffs, original_LDs):\n",
    "             # theta is an array. Need to turn it back into a dictionary.\n",
    "            theta_dict = {}\n",
    "            prior_prob = 0 # 0 or inf for uniform, else from gaussian\n",
    "            \n",
    "            if \"LD_coeff\" in modified_keys:\n",
    "                LD_coeffs = []\n",
    "            for theta_arr_item, modified_key in zip(theta, modified_keys):\n",
    "                if \"LD_coeff\" not in modified_key:\n",
    "                    theta_dict[modified_key] = theta_arr_item\n",
    "                else:\n",
    "                    LD_coeffs.append(theta_arr_item)\n",
    "            if \"LD_coeff\" in modified_keys:\n",
    "                theta_dict[\"LD_coeffs\"] = LD_coeffs\n",
    "            \n",
    "            a = theta_dict[\"a\"]\n",
    "            b = theta_dict[\"b\"]\n",
    "            params.rp = theta_dict[\"rp\"]\n",
    "            \n",
    "            # Now need to check the posteriors for all of these.\n",
    "            checks_on_posteriors = [\"T\"]\n",
    "            if not -5 < a < 5:\n",
    "                checks_on_posteriors.append(\"F\")\n",
    "                \n",
    "            if not -5 < b < 5:\n",
    "                checks_on_posteriors.append(\"F\")\n",
    "                \n",
    "            if  not 0.01 < theta_dict[\"rp\"] < 100:\n",
    "                checks_on_posteriors.append(\"F\")\n",
    "            \n",
    "            if using_kipping:\n",
    "                umin, umax = (0, 1)\n",
    "            elif (exoplanet_params[\"model_type\"] == \"quadratic\" and not using_kipping):\n",
    "                umin, umax = (-1, 2)\n",
    "            else:\n",
    "                umin, umax = (-5, 5)\n",
    "            if \"LD_coeff\" in modified_keys:\n",
    "                for u in LD_coeffs:\n",
    "                    if not umin <= u <= umax:\n",
    "                        checks_on_posteriors.append(\"F\")\n",
    "            \n",
    "            for key in (\"ecc\",\"period\",\"impact\",\"lop\",\"aoR\",\"offset\"):\n",
    "                if key in modified_keys:\n",
    "                    if priors_type == \"uniform\":\n",
    "                        if not priors_dict[key][0] <= theta_dict[key] <= priors_dict[key][1]:\n",
    "                            checks_on_posteriors.append(\"F\")\n",
    "                    if priors_type == \"gaussian\":\n",
    "                        gauss_mu = priors_dict[key][0]\n",
    "                        gauss_sig = priors_dict[key][1]\n",
    "                        prior_prob += np.log(1.0/(np.sqrt(2*np.pi)*gauss_sig))-0.5*(theta_dict[key]-gauss_mu)**2/gauss_sig**2\n",
    "            \n",
    "            if \"F\" not in checks_on_posteriors:\n",
    "                return prior_prob\n",
    "            return -np.inf\n",
    "\n",
    "        # Defines the probability.\n",
    "        def log_probability(theta, modified_keys, num_of_ld_coeffs, x, y, yerr, original_LDs):\n",
    "            lp = log_prior(theta, modified_keys, num_of_ld_coeffs, original_LDs)\n",
    "            if not np.isfinite(lp):\n",
    "                return -np.inf\n",
    "            return lp + log_likelihood(theta, modified_keys, num_of_ld_coeffs, x, y, yerr, original_LDs)\n",
    "\n",
    "        pos = theta_arr + 1e-3 * np.random.randn(N_walkers, theta_arr.shape[0])\n",
    "        #pos = theta_arr + (1+err[0]*np.random.randn(N_walkers, theta_arr.shape[0]))\n",
    "        nwalkers, ndim = pos.shape\n",
    "        print(\"Fitting %.0f parameters to data...\" % ndim)\n",
    "        original_LDs = deepcopy(params.u)\n",
    "        sampler = emcee.EnsembleSampler(\n",
    "            nwalkers, ndim, log_probability, args=(modified_keys, num_of_ld_coeffs, t, lc, np.array(err), original_LDs,),)\n",
    "            #moves=[(emcee.moves.DEMove(), 0.8),(emcee.moves.DESnookerMove(), 0.2),]\n",
    "        #)\n",
    "        sampler.run_mcmc(pos, N_steps, progress=True);\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(ndim, figsize=(10, 7), sharex=True)\n",
    "        samples = sampler.get_chain()\n",
    "        labels = [key for key in modified_keys]\n",
    "        for i in range(ndim):\n",
    "            ax = axes[i]\n",
    "            ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "            ax.set_xlim(0, len(samples))\n",
    "            ax.set_ylabel(labels[i])\n",
    "            ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "        axes[-1].set_xlabel(\"step number\")\n",
    "\n",
    "        n = np.shape(samples[:, :, 1])[0]*np.shape(samples[:, :, 1])[1]\n",
    "        fig, axes = plt.subplots(ndim, figsize=(10, 7), sharex=False)\n",
    "        samples = sampler.get_chain()\n",
    "        labels = [key for key in modified_keys]\n",
    "        for i in range(ndim):\n",
    "            ax = axes[i]\n",
    "            post = np.reshape(samples[:, :, i], (n))\n",
    "            ax.hist(post, 100, alpha=0.3)\n",
    "            ax.set_xlim(min(post), max(post))\n",
    "            ax.set_ylabel(labels[i])\n",
    "\n",
    "        flat_samples = sampler.get_chain(discard=int(0.2*N_steps), flat=True)\n",
    "        theta = []\n",
    "        posteriors = []\n",
    "        for i in range(ndim):\n",
    "            theta.append(np.percentile(flat_samples[:, i], 50))\n",
    "            posteriors.append(flat_samples[:, i])\n",
    "        \n",
    "        # Need to turn theta back into a dict.\n",
    "        theta_dict = {}\n",
    "        if \"LD_coeff\" in modified_keys:\n",
    "            LD_coeffs = []\n",
    "        for theta_arr_item, modified_key in zip(theta, modified_keys):\n",
    "            if \"LD_coeff\" not in modified_key:\n",
    "                theta_dict[modified_key] = theta_arr_item\n",
    "            else:\n",
    "                LD_coeffs.append(theta_arr_item)\n",
    "        if \"LD_coeff\" in modified_keys:\n",
    "            theta_dict[\"LD_coeffs\"] = LD_coeffs\n",
    "        theta = theta_dict\n",
    "        \n",
    "        fig = corner.corner(\n",
    "            flat_samples, labels=labels\n",
    "        );\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        plt.close()\n",
    "        \n",
    "        # Reinitialize model.\n",
    "        params = batman.TransitParams()\n",
    "        params.t0 = exoplanet_params[\"t0\"]                       #time of inferior conjunction in days\n",
    "        params.per = exoplanet_params[\"period\"]                  #orbital period in days\n",
    "        params.rp = exoplanet_params[\"rp\"]                       #planet radius (in units of stellar radii)\n",
    "        params.a = exoplanet_params[\"aoR\"]                       #semi-major axis (in units of stellar radii)\n",
    "        params.inc = (180/np.pi)*np.arccos(exoplanet_params[\"impact\"]/params.a)#exoplanet_params[\"inc\"]                     #orbital inclination (in degrees)\n",
    "        params.ecc = exoplanet_params[\"ecc\"]                     #eccentricity\n",
    "        params.w = exoplanet_params[\"lop\"]                       #longitude of periastron (in degrees)\n",
    "        params.u = exoplanet_params[\"LD_coeffs\"]                 #limb darkening coefficients [u1, u2]\n",
    "        params.limb_dark = exoplanet_params[\"model_type\"]        #limb darkening model\n",
    "        \n",
    "        # Replace default params with fitted params as applicable.\n",
    "        fit_params = theta.keys()\n",
    "        if \"LD_coeffs\" in fit_params:\n",
    "            params.u = theta[\"LD_coeffs\"]\n",
    "        if \"offset\" in fit_params:\n",
    "            params.u = [original_LDs[0], original_LDs[1] + theta[\"offset\"]]\n",
    "        if \"t0\" in fit_params:\n",
    "            params.t0 = theta[\"t0\"]\n",
    "        if \"period\" in fit_params:\n",
    "            params.per = theta[\"period\"]\n",
    "        if \"aoR\" in fit_params:\n",
    "            params.a = theta[\"aoR\"]\n",
    "        #if \"inc\" in fit_params:\n",
    "        if \"impact\" in fit_params:\n",
    "            params.inc = (180/np.pi)*np.arccos(theta[\"impact\"]/params.a)#theta[\"inc\"]\n",
    "        if \"ecc\" in fit_params:\n",
    "            params.ecc = theta[\"ecc\"]\n",
    "        if \"lop\" in fit_params:\n",
    "            params.w = theta[\"lop\"]\n",
    "        params.rp=theta[\"rp\"]\n",
    "        #print(params.u,params.a,params.per,params.inc,params.t0)\n",
    "        \n",
    "        m = batman.TransitModel(params, t)    #initializes model\n",
    "        polyfit= theta[\"a\"] + theta[\"b\"]*t\n",
    "        model = m.light_curve(params)*polyfit\n",
    "\n",
    "        residuals = lc - model\n",
    "        err = np.std(residuals)\n",
    "        print(\"Standard deviation of the residuals: %.0f ppm\" % (err*10**6))\n",
    "        \n",
    "        if depth_type == \"all\":\n",
    "            depth_types = [\"rp/rs\", \"standard\", \"ldcta\"]\n",
    "        else:\n",
    "            depth_types = [depth_type]\n",
    "        depths = []\n",
    "        depth_errs = []\n",
    "        for depth_type in depth_types:\n",
    "            if depth_type == \"rp/rs\":\n",
    "                depth = 100*theta[\"rp\"]\n",
    "                depth_err = depth_err = 100*np.std(posteriors[2])\n",
    "            if depth_type == \"standard\":\n",
    "                depth = 100*theta[\"rp\"]**2\n",
    "                depth_err = 100*2*theta[\"rp\"]*np.std(posteriors[2])\n",
    "            if depth_type == \"ldcta\":\n",
    "                # Code from Ryan MacDonald.\n",
    "                # Compute angles from star-planet line to R_p = R_s intersection\n",
    "                if \"aoR\" in theta.keys():\n",
    "                    aoR = theta[\"aoR\"]\n",
    "                else:\n",
    "                    aoR = exoplanet_params[\"aoR\"]\n",
    "                #if \"inc\" in theta.keys():\n",
    "                if \"impact\" in theta.keys():\n",
    "                    inc = (180/np.pi)*np.arccos(theta[\"impact\"]/aoR)#theta[\"inc\"]\n",
    "                else:\n",
    "                    inc = (180/np.pi)*np.arccos(exoplanet_params[\"impact\"]/aoR)#exoplanet_params[\"inc\"]\n",
    "                bo  = aoR*np.cos(inc*np.pi/180)\n",
    "                bo_sq = bo**2\n",
    "                rs = 1\n",
    "                rs_sq = rs**2\n",
    "                rp = theta[\"rp\"]\n",
    "                rp_sq = rp**2\n",
    "\n",
    "                arg_phi_1 = ((bo_sq * rs_sq) + rp_sq - rs_sq)/(2 * (bo * rs) * rp)\n",
    "                arg_phi_2 = ((bo_sq * rs_sq) + rs_sq - rp_sq)/(2 * (bo * rs) * rs)\n",
    "\n",
    "                phi_1 = np.arccos(arg_phi_1)  # Angle at planet centre\n",
    "                phi_2 = np.arccos(arg_phi_2)  # Angle at star centre\n",
    "\n",
    "                # Evaluate the overlapping area analytically\n",
    "                A_overlap = (rp_sq * (phi_1 - 0.5 * np.sin(2.0 * phi_1)) +\n",
    "                             rs_sq * (phi_2 - 0.5 * np.sin(2.0 * phi_2)))\n",
    "                A_s = np.pi*rs_sq\n",
    "                depth = 100*A_overlap/A_s\n",
    "\n",
    "                # The tedious process of computing the depth error\n",
    "                dphi_1 = (rs_sq*(bo_sq-1)-rp_sq)/(rp*np.sqrt(rs_sq*rs_sq*(-(bo_sq-1)**2)+2*rs_sq*(bo_sq+1)*rp_sq-rp_sq*rp_sq))\n",
    "                dphi_2 = 2*rp/(np.sqrt(rs_sq*rs_sq*(-(bo_sq-1)**2)+2*rs_sq*(bo_sq+1)*rp_sq-rp_sq*rp_sq))\n",
    "                depth_err = 100*(2*rp*(phi_1-0.5*np.sin(2 * phi_1))\n",
    "                                 + rp_sq*dphi_1*(1-np.cos(2 * phi_1))\n",
    "                                 + rs_sq*dphi_2*(1-np.cos(2 * phi_2)))*np.std(posteriors[2])\n",
    "            depths.append(depth)\n",
    "            depth_errs.append(depth_err)\n",
    "        if len(depths) == 1:\n",
    "            return theta, depths[0], depth_errs[0], model, residuals, err\n",
    "        else:\n",
    "            return theta, depths, depth_errs, model, residuals, err\n",
    "    except ValueError:\n",
    "        print(\"Encountered value error, returning Nones...\")\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_outliers(curve, timestamps, residuals, sigma, raise_alarm):\n",
    "    res_mean = np.mean(residuals)\n",
    "    res_sig = np.std(residuals)\n",
    "    outliers = np.where(np.abs(residuals-res_mean) > sigma*res_sig)[0]\n",
    "    print(\"{} outliers were deleted at these times: \".format(len(outliers)), timestamps[outliers])\n",
    "    if len(outliers) >= raise_alarm:\n",
    "        print(\"alarm!\")\n",
    "    curve = np.delete(curve, outliers)\n",
    "    timestamps = np.delete(timestamps, outliers)\n",
    "    return curve, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transit_spectrum(wavelengths, depths, depth_errs, ymin=0, ymax=4):\n",
    "    # Generates and saves a plot of the transit spectrum.\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    prox = [i for i in range(len(depths))]\n",
    "    ax.scatter(prox, depths, s=5, color=\"k\", marker=\"s\")\n",
    "    ax.errorbar(prox, depths, yerr=depth_errs, fmt=\"none\", capsize=0, ecolor=\"k\", elinewidth=3)\n",
    "    \n",
    "    ax.set_xlabel(\"wavelength [micron]\", fontsize=14)\n",
    "    ax.set_ylabel(\"transit depth [percent]\", fontsize=14)\n",
    "    #ax.set_xlim(min(wavelengths), max(wavelengths))\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.xaxis.set_major_formatter(fsf(\"%.1f\"))\n",
    "    ax.yaxis.set_major_formatter(fsf(\"%.2f\"))\n",
    "    ax.xaxis.set_minor_formatter(nulf())\n",
    "    ax.xaxis.set_minor_locator(aml(4))\n",
    "    \n",
    "    sparse_prox = [i for i in range(0, len(depths), 10)]\n",
    "    ax.set_xticks(sparse_prox)\n",
    "    xticklabels=[]\n",
    "    wavelengths = np.round(np.array(wavelengths),decimals=3)\n",
    "    for tick in sparse_prox:\n",
    "        xticklabels.append(str(wavelengths[tick]))\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    \n",
    "    ax.tick_params(which=\"both\", axis=\"both\", direction=\"in\", pad=5, labelsize=10)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transit_spectrum(wavelengths, halfwidths, depths, depth_errs, outdir):\n",
    "    # Assuming bins of constant halfwidth, writes the transit spectrum out as\n",
    "    # a POSEIDON-compatible .txt file.\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    with open('{}/transit_spectrum.txt'.format(outdir), mode='w') as file:\n",
    "        file.write(\"#wavelength[micron]     halfwidth[micron]     depth[na]     uncertainty[na]\\n\")\n",
    "        for wav, hw, depth, depth_err in zip(wavelengths, halfwidths, depths, depth_errs):\n",
    "            file.write('{:5.4}   {:5.4}   {:8.6}   {:8.6}\\n'.format(wav, hw, depth/100, depth_err/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856ba86",
   "metadata": {},
   "source": [
    "Everything down here is for you to edit! It is for running the scripts above on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e59de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Stage 1 calibrations on the file located at filepath. Output file outfile will be sent to outdir.\n",
    "filepath = \"./JWST_WD1856-selected/jw02358030001_04101_00001-seg001_nrs1_uncal.fits\"\n",
    "outfile = \"WD1856\" # will have suffix _rateints.fits attached to it when saved\n",
    "outdir = \"./rateints\"\n",
    "\n",
    "doStage1(filepath, outfile, outdir,\n",
    "         group_scale={\"skip\":False},\n",
    "         dq_init={\"skip\":False},\n",
    "         saturation={\"skip\":False},\n",
    "         superbias={\"skip\":False},\n",
    "         refpix={\"skip\":False},\n",
    "         linearity={\"skip\":False},\n",
    "         dark_current={\"skip\":False},\n",
    "         jump={\"skip\":True},\n",
    "         ramp_fit={\"skip\":False},\n",
    "         gain_scale={\"skip\":False},\n",
    "         one_over_f={\"skip\":False, \"bckg_rows\":[1,2,3,4,5,6,-1,-2,-3,-4,-5,-6], \"show\":False}\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stage 2 calibrations on the file located at filepath. Output file outfile will be sent to outdir.\n",
    "datadir = \"./rateints\"\n",
    "filepaths = sorted(glob.glob(os.path.join(datadir, \"*_rateints.fits\")))\n",
    "outdir = \"./calints\"\n",
    "\n",
    "for filepath in filepaths:\n",
    "    outfile = str.replace(filepath, \"rateints\", \"calints\")\n",
    "    doStage2(filepath, outfile, outdir,\n",
    "             assign_wcs={\"skip\":False},\n",
    "             extract_2d={\"skip\":False},\n",
    "             srctype={\"skip\":False},\n",
    "             wavecorr={\"skip\":False},\n",
    "             flat_field={\"skip\":True},\n",
    "             pathloss={\"skip\":True},\n",
    "             photom={\"skip\":True},\n",
    "             resample_spec={\"skip\":True},\n",
    "             extract_1d={\"skip\":True}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20541ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Stage 3 calibrations on all *_calints.fits files located at filepath. Output file outfile will be sent to outdir.\n",
    "filesdir = \"./calints\"\n",
    "outdir = \"./postprocessed\"\n",
    "\n",
    "hcut1 = 8\n",
    "hcut2 = 17\n",
    "\n",
    "frames_to_reject = []\n",
    "\n",
    "doStage3(filesdir, outdir,\n",
    "         trace_aperture={\"hcut1\":hcut1,\n",
    "                         \"hcut2\":hcut2,\n",
    "                         \"vcut1\":0,\n",
    "                         \"vcut2\":432},\n",
    "         frames_to_reject = frames_to_reject,\n",
    "         loss_stats_step={\"skip\":False},\n",
    "         mask_flagged_pixels={\"skip\":False},\n",
    "         iteration_outlier_removal={\"skip\":False, \"n\":2, \"sigma\":10},\n",
    "         spatialfilter_outlier_removal={\"skip\":False, \"sigma\":4.5, \"kernel\":(1,15)},\n",
    "         laplacianfilter_outlier_removal={\"skip\":True, \"sigma\":10},\n",
    "         second_bckg_subtract={\"skip\":False,\"bckg_rows\":[0,1,2,-2,-1]},\n",
    "         track_source_location={\"skip\":False,\"reject_disper\":False,\"reject_spatial\":False}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce wavbins with specified R, wavmin, wavmax.\n",
    "wavmin = 0.6\n",
    "wavmax = 5.3\n",
    "R = 50\n",
    "\n",
    "you_can_use_these_bins = []\n",
    "l = wavmin\n",
    "while l <= wavmax:\n",
    "    you_can_use_these_bins.append(l)\n",
    "    l = l + l/R\n",
    "\n",
    "you_can_use_these_bins = np.round(you_can_use_these_bins, 3)\n",
    "#you_can_use_these_bins = np.array([0.523, 0.883])\n",
    "print(you_can_use_these_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stage 4 extractions on the file located at filepath. Output files will be sent to outdir.\n",
    "filepaths = sorted(glob.glob(os.path.join(outdir, \"postprocessed_*\")))\n",
    "outdir = \"./spectra_Blouin\"\n",
    "\n",
    "wavbins = you_can_use_these_bins\n",
    "epoch = 60061.50686491065 # MJD\n",
    "\n",
    "doStage4(filepaths, outdir,\n",
    "         trace_aperture={\"hcut1\":hcut1,\n",
    "                         \"hcut2\":hcut2,\n",
    "                         \"vcut1\":0,\n",
    "                         \"vcut2\":432},\n",
    "         extract_light_curves={\"skip\":False,\n",
    "                               \"wavbins\":wavbins,\n",
    "                               \"ext_type\":\"box\"},\n",
    "         median_normalize_curves={\"skip\":False},\n",
    "         sigma_clip_curves={\"skip\":True,\n",
    "                            \"b\":100,\n",
    "                            \"clip_at\":5},\n",
    "         fix_transit_times={\"skip\":False,\n",
    "                            \"epoch\":epoch},\n",
    "         plot_light_curves={\"skip\":False},\n",
    "         save_light_curves={\"skip\":False}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f5814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Stage 5 light curve fitting on the .txt files located in filesdir. Output files will be sent to outdir.\n",
    "filesdir = os.path.join(\"./spectra_Blouin\", \"output_txts_extraction\")\n",
    "outdir = \"./spectra_Blouin\"\n",
    "\n",
    "# Set path to custom LD model.\n",
    "path_to_custom_LD_model = \"/Users/abby/code_dev/jwst/WD_1856/blouin_WD1856_LDmodel/Imu_bestfitJWST.txt\"\n",
    "\n",
    "# Initialize exoplanet_params.\n",
    "exoplanet_params = {}\n",
    "exoplanet_params[\"period\"] = 1.407939217\n",
    "exoplanet_params[\"t0\"] = 0\n",
    "exoplanet_params[\"rp\"] = 7.28\n",
    "exoplanet_params[\"aoR\"] = 335.22 #349.83347059561754\n",
    "#exoplanet_params[\"inc\"] = 88.778\n",
    "exoplanet_params[\"impact\"] = 7.26 #7.92797143651862\n",
    "exoplanet_params[\"ecc\"] = 0.0\n",
    "exoplanet_params[\"lop\"] = 90\n",
    "exoplanet_params[\"offset\"] = 0.0\n",
    "\n",
    "# Initialize systematics guess.\n",
    "systematics = (1,0) # (b, m) for mx + b\n",
    "\n",
    "# Input full wavelength range.\n",
    "spectral_range = (np.min(you_can_use_these_bins),np.max(you_can_use_these_bins))\n",
    "\n",
    "doStage5(filesdir, outdir, exoplanet_params, systematics, spectral_range,\n",
    "         do_fit={\"WLC_LM\":True,\n",
    "                 \"WLC_MCMC\":True,\n",
    "                 \"spec_LM\":True,\n",
    "                 \"spec_MCMC\":True,},\n",
    "         limb_darkening_model={\"model_type\":\"quadratic\",\n",
    "                               \"stellar_params\":None,\n",
    "                               \"initial_guess\":[0,0],},\n",
    "         fixed_param={\"LD_coeffs\":True,\n",
    "                      \"t0\":False,\n",
    "                      \"period\":True,\n",
    "                      \"aoR\":False,\n",
    "                      \"impact\":False,#\"inc\":False,\n",
    "                      \"ecc\":True,\n",
    "                      \"lop\":True,\n",
    "                      \"offset\":False},\n",
    "         MCMC_depth_type=\"all\",\n",
    "         priors_dict={\"t0\":[0.0, 0.001],\n",
    "                      \"period\":[1.407939217, 0.000000016], #1.407939217 from ttv, 1.4079389 from wd 1856 proposal\n",
    "                      \"aoR\":[336, 14],\n",
    "                      \"impact\":[7.16, 0.65],#\"inc\":[88.778, 0.059],\n",
    "                      \"ecc\":[0, 0.8],\n",
    "                      \"lop\":[0, 90],\n",
    "                      \"offset\":[0.2,0.25]},\n",
    "         priors_type=\"gaussian\",\n",
    "         reject_threshold=2.5,\n",
    "         raise_alarm=10,\n",
    "         exoticLD={\"available\":True,\n",
    "                   \"ld_data_path\":\"/Users/abby/opt/anaconda3/exotic_ld_data-3.1.2\",\n",
    "                   \"ld_grid\":'custom',\n",
    "                   \"ld_interpolate_type\":'trilinear',\n",
    "                   \"custom_model_path\":path_to_custom_LD_model},\n",
    "         save_plots={\"WLC_LM\":True,\n",
    "                     \"WLC_MCMC\":True,\n",
    "                     \"spec_LM\":True,\n",
    "                     \"spec_MCMC\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12118d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
